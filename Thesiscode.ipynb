{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! STEP1: In terminal:\n",
    "#! ThesisLocal/Scripts/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always Import\n",
    "import mne\n",
    "from mne.filter import filter_data\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "mne.set_log_level(30) # level of 10 prints all info, level of 30 only prints warnings and errors\n",
    "\n",
    "# Pre-Processing\n",
    "import time\n",
    "import picard\n",
    "import joblib\n",
    "from mne.preprocessing import ICA\n",
    "from IPython.display import clear_output\n",
    "import gc\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Everything PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import models, transforms\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Alexnet\n",
    "from torchvision.models import alexnet \n",
    "\n",
    "# import optuna\n",
    "# from optuna.integration import PyTorchLightningPruningCallback\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For old tensorflow model\n",
    "from EEGModels import EEGNet, DeepConvNet\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical, Sequence\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "from braindecode.models import EEGNetv4, Deep4Net\n",
    "from braindecode.datasets import create_from_X_y\n",
    "from braindecode import EEGClassifier\n",
    "from skorch import NeuralNetClassifier, NeuralNetBinaryClassifier\n",
    "from skorch.callbacks import LRScheduler, EarlyStopping, ProgressBar, EpochScoring\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.dataset import ValidSplit\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split, cross_val_score\n",
    "from skorch.classifier import NeuralNetBinaryClassifier\n",
    "from skorch.helper import SliceDataset\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "from torch.utils.data import random_split\n",
    "from torcheeg.models import EEGNet as torchEEG\n",
    "from torcheeg.trainers import ClassifierTrainer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dict = {\"block_start\": 1, \"block_end\": 2} # Preparing the event ID's as per the documentation (EEG experiment protocol_Migraine_Brain_KiltHub_vF.pdf)\n",
    "variances = []\n",
    "ica = ICA(n_components=50, max_iter='auto', random_state=97) # choosing 50 components, explains at least 95% of variance or more in every file while minimizing convergence time from ~10mins to less than 1min in most cases\n",
    "\n",
    "reject_criteria = dict(\n",
    "    eeg=75e-6,  # 100 µV\n",
    "    eog=200e-6,\n",
    "    ecg=1200e-6\n",
    ")  # 200 µV\n",
    "\n",
    "flat_criteria = dict(eeg=1e-6)  # 1 fT  # 1 fT/cm  # 1 µV\n",
    "\n",
    "\n",
    "for file in os.listdir('Dataset')[2:]: #! WARNING: This script may currently not work as intended again due to a change in file naming convention, be aware\n",
    "    shortname = file[:-12]\n",
    "    \n",
    "    print(f'Now working on {file}')\n",
    "    raw = mne.io.read_raw_bdf(f\"Dataset/{file}\", preload=True)\n",
    "    raw_unfiltered = raw.copy()\n",
    "    current_sfreq = raw.info[\"sfreq\"]\n",
    "    desired_sfreq = 256  # Hz\n",
    "    decim = np.round(current_sfreq / desired_sfreq).astype(int)\n",
    "    obtained_sfreq = current_sfreq / decim\n",
    "    lowpass_freq = obtained_sfreq / 3.0\n",
    "    raw = raw.filter(l_freq=0.5, h_freq=lowpass_freq)\n",
    "    raw = raw.resample(256)\n",
    "    raw = raw.filter(l_freq=0.5, h_freq=100, method='iir') #(i) zero-phase Butterworth filter was used to filter the signals between 0.1 and 100 Hz;\n",
    "    raw = raw.set_channel_types({'ECG':'ecg', 'Fp1':'eog', 'Fp2':'eog', 'Fpz':'eog'})\n",
    "\n",
    "    eog_epochs = mne.preprocessing.create_eog_epochs(raw)\n",
    "    ecg_epochs = mne.preprocessing.create_ecg_epochs(raw)\n",
    "    \n",
    "    print(f'Now Resampling and Epoching {file}')\n",
    "    raw = raw.pick(picks=None, exclude=['Status'])\n",
    "    raw = mne.make_fixed_length_epochs(raw, duration=2, overlap=0)\n",
    "    raw.load_data()\n",
    "    raw = raw.apply_baseline((None, None))\n",
    "    \n",
    "    # ecg_events = mne.preprocessing.find_ecg_events(raw)\n",
    "    # onsets = ecg_events[0][:, 0] / raw.info[\"sfreq\"] - 0.25    \n",
    "    # durations = [0.5] * len(ecg_events)\n",
    "    # descriptions = [\"bad beat\"] * len(ecg_events)\n",
    "    # beat_annot = mne.Annotations(\n",
    "    #     onsets, durations, descriptions, orig_time=raw.info[\"meas_date\"]\n",
    "    # )\n",
    "    # raw.set_annotations(beat_annot)   \n",
    "    # print(f'ECG events annotated')    \n",
    "     # setting up pre-existing channel ECG for heartbeat detection; Fp1 as eye-near channel is used as proxy for eye-related artifacts\n",
    "    raw = raw.set_eeg_reference(['M1', 'M2'])\n",
    "\n",
    "    print(f'Initializing ICA for {file}')\n",
    "    ica_filt_raw = raw_unfiltered.copy().filter(l_freq=1.0, h_freq=None)\n",
    "    ica_filt_raw.set_channel_types({'ECG':'ecg', 'Fp1':'eog', 'Fp2':'eog', 'Fpz':'eog'})#(iii) independent component analysis was used to identify and remove eye-related artefacts (blinks and horizontal eye movements) and heartbeat\n",
    "    ica.fit(ica_filt_raw, decim=decim)\n",
    "    \n",
    "    explained_var_ratio = ica.get_explained_variance_ratio(ica_filt_raw)\n",
    "    for ratio in explained_var_ratio.items():\n",
    "        variances.append(ratio)\n",
    "    # Finding out which components to exclude\n",
    "    ica.exclude = []\n",
    "    # find which ICs match the EOG/ECG patterns\n",
    "    eog_indices, eog_scores = ica.find_bads_eog(eog_epochs, threshold='auto', measure='zscore')\n",
    "    ica.exclude = eog_indices\n",
    "    ecg_indices, ecg_scores = ica.find_bads_ecg(ecg_epochs, threshold=\"auto\", measure='zscore')\n",
    "    ica.exclude += ecg_indices\n",
    "    raw = ica.apply(raw)\n",
    " \n",
    "    raw = raw.set_channel_types({'ECG':'eeg', 'Fp1':'eeg', 'Fp2':'eeg', 'Fpz':'eeg'})\n",
    "    scaler = mne.decoding.Scaler(info=raw.info, scalings=None)\n",
    "    raw.save(f'PreprocessedEpochs/{shortname}-epo.fif', overwrite=True)\n",
    "    fixed_epoch_data = raw.get_data()\n",
    "    fixed_epoch_data = scaler.fit_transform(fixed_epoch_data)\n",
    "    np.save(f'PreprocessedEpochData/{shortname}.npy', fixed_epoch_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final data to Pandas DF (Deprecated, ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframes = []\n",
    "\n",
    "for file in os.listdir('PreprocessedEpochs/Train'):\n",
    "    dfs = mne.read_epochs(f'PreprocessedEpochs/Train/{file}').to_data_frame()\n",
    "    dfs = dfs.drop(columns=['time', 'condition'])\n",
    "    dfs = dfs.groupby('epoch').mean().reset_index()\n",
    "    dfs = dfs.drop(columns=['epoch'])\n",
    "    \n",
    "    if file[0] == \"M\":\n",
    "        dfs['migraine'] = 1\n",
    "    else:\n",
    "        dfs['migraine'] = 0\n",
    "\n",
    "    train_dataframes.append(dfs)\n",
    "\n",
    "train_df = pd.concat(train_dataframes, ignore_index=True)\n",
    "\n",
    "test_dataframes = []\n",
    "\n",
    "for file in os.listdir('PreprocessedEpochs/Test'):\n",
    "    dfs = mne.read_epochs(f'PreprocessedEpochs/Test/{file}').to_data_frame()\n",
    "    dfs = dfs.drop(columns=['time', 'condition'])\n",
    "    dfs = dfs.groupby('epoch').mean().reset_index()\n",
    "    dfs = dfs.drop(columns=['epoch'])\n",
    "    \n",
    "    if file[0] == \"M\":\n",
    "        dfs['migraine'] = 1\n",
    "    else:\n",
    "        dfs['migraine'] = 0\n",
    "\n",
    "    test_dataframes.append(dfs)\n",
    "\n",
    "test_df = pd.concat(test_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING THE DIFFERENT BRAINREGIONS TO REDUCE FEATURES\n",
    "df = pd.read_csv(\"FullDF.csv\", index_col=0)\n",
    "\n",
    "regions = {\n",
    "    'AF_left': ['AF7','AF3'],\n",
    "    'AF_center': ['AFz'],\n",
    "    'AFp_left': ['AFp1'],\n",
    "    'AFp_right': ['AFp2'],\n",
    "    'AF_right': ['AF4', 'AF8'],\n",
    "    'AFF_left': ['AFF5h', 'AFF1h'],\n",
    "    'AFF_right': ['AFF2h', 'AFF6h'],\n",
    "    \n",
    "    'C_left': ['C5','C3','C1'],\n",
    "    'C_center': ['Cz'],\n",
    "    'C_right': ['C2','C4','C6'],  \n",
    "    'CCP_left': ['CCP5h','CCP3h','CCP1h'],\n",
    "    'CCP_right': ['CCP2h','CCP4h','CCP6h'],\n",
    "    'CP_left': ['CP5','CP3','CP1'],\n",
    "    'CP_center': ['CPz'],\n",
    "    'CP_right': ['CP2','CP4','CP6'],\n",
    "    'CPP_left': ['CPP5h','CPP3h','CPP1h'],\n",
    "    'CPP_right': ['CPP2h','CPP4h','CPP6h'],\n",
    "    \n",
    "    'Erg_left': ['Erg1'],\n",
    "    'Erg_right': ['Erg2'],\n",
    "        \n",
    "    'F_left': ['F9','F7','F5','F3','F1'],\n",
    "    'F_center': ['Fz'],\n",
    "    'F_right': ['F2','F4','F6','F8','F10'],\n",
    "    'FC_left': ['FC5','FC3','FC1'],\n",
    "    'FC_center': ['FCz'],\n",
    "    'FC_right': ['FC2','FC4','FC6'],\n",
    "    'FCC_left': ['FCC5h', 'FCC3h', 'FCC1h'],\n",
    "    'FCC_right': ['FCC2h', 'FCC4h', 'FCC6h'],\n",
    "    'FFC_left': ['FFC5h', 'FFC3h', 'FFC1h'],\n",
    "    'FFC_right': ['FFC2h', 'FFC4h', 'FFC6h'],\n",
    "    'FFT_left': ['FFT9h', 'FFT7h'],\n",
    "    'FFT_right': ['FFT8h', 'FFT10h'],\n",
    "    'Fp1_left': ['Fp1'],\n",
    "    'Fp_center': ['Fpz'],\n",
    "    'Fp_right': ['Fp2'],\n",
    "    'FT_left': ['FT9', 'FT7'],\n",
    "    'FT_right': ['FT8', 'FT10'],\n",
    "    'FTT_left': ['FTT9h', 'FTT7h'],\n",
    "    'FTT_right': ['FTT8h', 'FTT10h'],\n",
    "    \n",
    "    'GSR_left': ['GSR1'],\n",
    "    'GSR_right': ['GSR2'],\n",
    "    \n",
    "    'I_left': ['I1'],\n",
    "    'I_center': ['Iz'],\n",
    "    'I_right': ['I2'],\n",
    "    'IO_left': ['IO1'],\n",
    "    'IO_right': ['IO2'],       \n",
    "    \n",
    "    'LO_left': ['LO1'],\n",
    "    'LO_right': ['LO2'],\n",
    "    \n",
    "    'M_left': ['M1'],\n",
    "    'M_right': ['M2'],\n",
    "    \n",
    "    'O_left': ['O1'],\n",
    "    'O_center': ['Oz'],\n",
    "    'O_right': ['O2'],\n",
    "    \n",
    "    'OI_left': ['OI1h'],\n",
    "    'OI_right': ['OI2h'],    \n",
    "    \n",
    "    'P_left': ['P9','P7','P5','P3','P1'],\n",
    "    'P_center': ['Pz'],\n",
    "    'P_right': ['P2','P4','P6','P8','P10'],\n",
    "    'PO_left': ['PO7', 'PO3', 'PO9'],\n",
    "    'PO_center': ['POz'],\n",
    "    'PO_right': ['PO4', 'PO8', 'PO10'],\n",
    "    'POO_left': ['POO1'],\n",
    "    'POO_right': ['POO2'],    \n",
    "    'POOh_left': ['POO9h'],\n",
    "    'POOh_right': ['POO10h'],\n",
    "    'PPO_left': ['PPO9h', 'PPO5h', 'PPO1h'],\n",
    "    'PPO_right': ['PPO2h', 'PPO6h', 'PPO10h'], \n",
    "    \n",
    "    'SO_left': ['SO1'],   \n",
    "\n",
    "    'T_left': ['T7'],\n",
    "    'T_right': ['T8'],\n",
    "    'TP_left': ['TP9', 'TP7'],\n",
    "    'TP_right': ['TP8', 'TP10'],\n",
    "    'TPP_left': ['TPP7h'],\n",
    "    'TPP_right': ['TPP8h'],\n",
    "    'TTP_left': ['TTP7h'],\n",
    "    'TTP_right': ['TTP8h'],\n",
    "    \n",
    "    'Resp': ['Resp'],\n",
    "    'Plet': ['Plet'],\n",
    "    'Temp': ['Temp'],\n",
    "    'ECG': ['ECG']\n",
    "}\n",
    "\n",
    "# The next 10 lines have been supplemented by ChatGPT, but have been manually checked for correctness\n",
    "new_columns = {}\n",
    "for freq in ['delta','theta','alpha','beta','gamma']:\n",
    "    for group_name, channels in regions.items():        \n",
    "        cols_to_avg = [f\"{freq}_{ch}\" for ch in channels if f\"{freq}_{ch}\" in df.columns]\n",
    "        \n",
    "        if cols_to_avg:  # Only if there are matching columns\n",
    "            # Calculate the average for the current frequency and group\n",
    "            new_col_name = f\"{freq}_{group_name}\"\n",
    "            new_columns[new_col_name] = df[cols_to_avg].mean(axis=1)\n",
    "df_grouped = pd.DataFrame(new_columns)\n",
    "\n",
    "dropped_df = df.drop(df.columns[np.arange(705)], axis=1)\n",
    "df = pd.concat([dropped_df, df_grouped], axis=1)\n",
    "df.to_csv(\"GroupedDF.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_resting = os.listdir('PreprocessedEpochs')\n",
    "freqs = np.logspace(*np.log10([1, 100]), num=50)\n",
    "n_cycles = freqs / 2.0\n",
    "\n",
    "tfr_input= mne.read_epochs(f'PreprocessedEpochs/{all_resting[0]}', preload=True)\n",
    "power = tfr_input.compute_tfr(method='morlet', freqs=freqs, n_cycles=n_cycles, picks='all', verbose=10, decim=10, n_jobs=16)\n",
    "power.save(f'PreprocessedTFR/{all_resting[0]}-tfr.hdf5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "montage = mne.channels.make_standard_montage(\"standard_1020\")\n",
    "montage.plot()\n",
    "fig = montage.plot(kind=\"3d\", show=True)  # 3D\n",
    "fig = fig.gca().view_init(azim=70, elev=15)  # set view angle for tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = mne.io.read_raw_fif(\"Preprocessed/M1.raw.fif\")\n",
    "power = mne.time_frequency.read_tfrs(\"PreprocessedTFR/M1-tfr.hdf5\")\n",
    "#print(power[0].average().plot()) #power[x] chooses epoch, picks=y chooses channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATES MONTAGE FOR STANDARD BIOSEMI, HOWEVER THIS DATASET HAS A CUSTOM-MADE EEG CAP\n",
    "#mne.channels.get_builtin_montages(descriptions=True)\n",
    "montage = mne.channels.make_standard_montage('biosemi128')\n",
    "montage.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_resting = os.listdir('PreprocessedEpochs')\n",
    "freqs = np.logspace(*np.log10([1, 100]), num=50)\n",
    "n_cycles = freqs / 2.0\n",
    "\n",
    "for file in all_resting:\n",
    "    clear_output()\n",
    "    shortname = file[:-8]\n",
    "    if f'{shortname}-STFT-tfr.hdf5' not in os.listdir('PreprocessedTFR-STFT'):\n",
    "        print(f\"Now computing {shortname}\")\n",
    "        tfr_input= mne.read_epochs(f'PreprocessedEpochs/{file}', preload=True)\n",
    "        power = tfr_input.compute_tfr(method='multitaper', freqs=freqs, n_cycles=n_cycles, picks='all', verbose=10, decim=10, n_jobs=-1) #Methods: multitaper for STFT, morlet for CWT (replacing bump)\n",
    "        del tfr_input\n",
    "        power.save(f'PreprocessedTFR-STFT/{shortname}-STFT-tfr.hdf5')\n",
    "        del power\n",
    "        gc.collect()\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFR through AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortname = \"C1\"\n",
    "power = mne.time_frequency.read_tfrs(f'PreprocessedTFR/C1-tfr.hdf5') #reads in the TFR\n",
    "images = power[52].average().plot(picks=\"Cz\", show=False, colorbar=False, baseline=(None, None), mode=\"zlogratio\") \n",
    "Path.mkdir(f'AlexNet/{shortname}', exist_ok=True)\n",
    "image = images[0]\n",
    "ax = image.axes[0]\n",
    "ax.axis('off')\n",
    "image.savefig(f'AlexNet/{shortname}/temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"PreprocessedTFR\")\n",
    "print(files[32:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#channel_list = ['Fp1','AFp1','AF7','AF3','AFF5h','AFF1h','F9','F7','F5','F3','F1','FFT9h','FFT7h','FFC5h','FFC3h','FFC1h','FT9','FT7','FC5','FC3','FC1','FTT9h','FTT7h','FCC5h','FCC3h','FCC1h','T7','C5','C3','C1','TTP7h','CCP5h','CCP3h','CCP1h','TP9','TP7','CP5','CP3','CP1','CPz','TPP7h','CPP5h','CPP3h','CPP1h','P9','P7','P5','P3','P1','Pz','PPO9h','PPO5h','PPO1h','PO7','PO3','POz','PO9','POO9h','O1','POO1','I1','OI1h','Oz','Iz','Fpz','Fp2','AFp2','AFz','AF4','AF8','AFF2h','AFF6h','Fz','F2','F4','F6','F8','F10','FFC2h','FFC4h','FFC6h','FFT8h','FFT10h','FCz','FC2','FC4','FC6','FT8','FT10','FCC2h','FCC4h','FCC6h','FTT8h','FTT10h','Cz','C2','C4','C6','T8','CCP2h','CCP4h','CCP6h','TTP8h','CP2','CP4','CP6','TP8','TP10','CPP2h','CPP4h','CPP6h','TPP8h','P2','P4','P6','P8','P10','PPO2h','PPO6h','PPO10h','PO4','PO8','PO10','POO2','O2','POO10h','OI2h','I2','M1','M2','LO1','LO2','IO1','SO1','IO2','ECG','GSR1','GSR2','Erg1','Erg2','Resp','Plet','Temp','Status']\n",
    "short_channel_list = ['AF7', 'AF3', 'F7', 'F5', 'F3', 'F1', 'FC5', 'FC3', 'AF4', 'AF8', 'F2', 'F4', 'F6', 'F8', 'FC4', 'FC6', 'C6', 'CCP2h', 'CCP4h', 'TTP8h', 'CP6', 'TP10', 'CPP4h', 'TPP8h', 'CPP1h', 'CPP3h', 'CP5', 'CP1', 'CPP5h', 'CPP2h', 'P9', 'P5', 'P2', 'P6', 'PPO2h', 'PPO10h', 'PO4', 'PO10', 'POO10h', 'I2', 'Pz', 'PPO5h', 'PO7', 'PO3', 'POO9h', 'POO1', 'I1', 'Oz', 'O2', 'O1']\n",
    "\n",
    "# ! Just for documentation which channels were chosen\n",
    "# frontal_left = ['AF7', 'AF3', 'F7', 'F5', 'F3', 'F1', 'FC5', 'FC3']\n",
    "# frontal_right = ['AF4', 'AF8', 'F2', 'F4', 'F6', 'F8', 'FC4', 'FC6']\n",
    "# parietal_left = ['C6', 'CCP2h', 'CCP4h', 'TTP8h', 'CP6', 'TP10', 'CPP4h', 'TPP8h']\n",
    "# parietal_right= ['CPP1h', 'CPP3h', 'CP5', 'CP1', 'CPP5h', 'CPP2h', 'P9', 'P5']\n",
    "# occipito_parietal_left = ['P2', 'P6', 'PPO2h', 'PPO10h', 'PO4', 'PO10', 'POO10h', 'I2']\n",
    "# occipito_parietal_right = ['Pz', 'PPO5h', 'PO7', 'PO3', 'POO9h', 'POO1', 'I1', 'Oz']\n",
    "# occipital_left = ['O2']\n",
    "# occipital_right = ['O1']\n",
    "\n",
    "labels = pd.DataFrame(columns=['file', 'label', 'aura', 'old', 'female'])\n",
    "files = os.listdir(\"PreprocessedTFR\")\n",
    "files = files[32:]\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=Image.BICUBIC),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "aura = ['M1', 'M2', 'M4', 'M5', 'M6', 'M7', 'M10', 'M11', 'M14', 'M16', 'M17', 'M18']\n",
    "old = ['M6', 'M7', 'M8', 'M9', 'M14', 'M17', 'M18', 'C5', 'C7', 'C8', 'C9', 'C10', 'C17', 'C21']\n",
    "female = ['M2', 'M3', 'M5', 'M6', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16', 'M17', 'M18', 'C1', 'C3', 'C4', 'C8', 'C9', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21'] \n",
    "\n",
    "for file in files: #files contains the name of each subjects file\n",
    "    shortname = file[:-9] #extracts only the subject name (i.e., C1)\n",
    "    Path.mkdir(f'AlexNet/{shortname}', exist_ok=True)\n",
    "    power = mne.time_frequency.read_tfrs(f'PreprocessedTFR/{file}') #reads in the TFR\n",
    "    for i in random.sample(range(0,power.shape[0]), 50): # takes 50 random epochs across all epochs of a file\n",
    "        for channel in short_channel_list: #short_channel_list contains a selection of 50 channels that should cover all areas of the brain sufficiently and roughly equally\n",
    "            print(f\"now {shortname}-{i}-{channel}\")\n",
    "            images = power[i].average().plot(picks=channel, show=False, colorbar=False, baseline=(None, None), mode=\"zlogratio\") #baselined zlog image is plotted\n",
    "            image = images[0]\n",
    "            ax = image.axes[0]\n",
    "            ax.axis('off')\n",
    "            image.savefig(\"temp\")\n",
    "            matplotlib.pyplot.close()\n",
    "            img = Image.open(\"temp.png\").convert(\"RGB\") # image is converted into RGB rather than RGBT image\n",
    "            img = preprocess(img) # turned into tensor\n",
    "            torch.save(img, f=f'AlexNet/{shortname}/{shortname}-{i}-{channel}.pt')\n",
    "            #img.save(f'AlexNet/{shortname}/{shortname}-{i}-{channel}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_list = ['Fp1','AFp1','AF7','AF3','AFF5h','AFF1h','F9','F7','F5','F3','F1','FFT9h','FFT7h','FFC5h','FFC3h','FFC1h','FT9','FT7','FC5','FC3','FC1','FTT9h','FTT7h','FCC5h','FCC3h','FCC1h','T7','C5','C3','C1','TTP7h','CCP5h','CCP3h','CCP1h','TP9','TP7','CP5','CP3','CP1','CPz','TPP7h','CPP5h','CPP3h','CPP1h','P9','P7','P5','P3','P1','Pz','PPO9h','PPO5h','PPO1h','PO7','PO3','POz','PO9','POO9h','O1','POO1','I1','OI1h','Oz','Iz','Fpz','Fp2','AFp2','AFz','AF4','AF8','AFF2h','AFF6h','Fz','F2','F4','F6','F8','F10','FFC2h','FFC4h','FFC6h','FFT8h','FFT10h','FCz','FC2','FC4','FC6','FT8','FT10','FCC2h','FCC4h','FCC6h','FTT8h','FTT10h','Cz','C2','C4','C6','T8','CCP2h','CCP4h','CCP6h','TTP8h','CP2','CP4','CP6','TP8','TP10','CPP2h','CPP4h','CPP6h','TPP8h','P2','P4','P6','P8','P10','PPO2h','PPO6h','PPO10h','PO4','PO8','PO10','POO2','O2','POO10h','OI2h','I2','M1','M2','LO1','LO2','IO1','SO1','IO2','ECG','GSR1','GSR2','Erg1','Erg2','Resp','Plet','Temp','Status']\n",
    "labels = pd.DataFrame(columns=['file', 'label', 'aura', 'old', 'female'])\n",
    "files = os.listdir(\"PreprocessedTFR\")\n",
    "preprocess = transforms.Compose([transforms.Resize((227,227), interpolation=Image.BICUBIC)])\n",
    "\n",
    "aura = ['M1', 'M2', 'M4', 'M5', 'M6', 'M7', 'M10', 'M11', 'M14', 'M16', 'M17', 'M18']\n",
    "old = ['M6', 'M7', 'M8', 'M9', 'M14', 'M17', 'M18', 'C5', 'C7', 'C8', 'C9', 'C10', 'C17', 'C21']\n",
    "female = ['M2', 'M3', 'M5', 'M6', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16', 'M17', 'M18', 'C1', 'C3', 'C4', 'C8', 'C9', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21'] \n",
    "\n",
    "for file in files: #files contains the name of each subjects file\n",
    "    shortname = file[:-9] #extracts only the subject name (i.e., C1)\n",
    "    power = mne.time_frequency.read_tfrs(f'PreprocessedTFR/{file}') #reads in the TFR\n",
    "    for i in range(0,power.shape[0]+1): #power.shape[0] is the amount of epochs of that file\n",
    "        for channel in channel_list: #channel_list contains all existing channels\n",
    "            if f'{shortname}-{i}-{channel}.png' not in os.listdir(\"TFR-CWT Representations\"): #failsafe in case the kernel crashes\n",
    "                print(f\"Now: {shortname}-{i}-{channel}\")\n",
    "                images = power[i].average().plot(picks=channel, show=False, colorbar=False, baseline=(None, None), mode=\"ratio\") \n",
    "                image = images[0]\n",
    "                ax = image.axes[0]\n",
    "                ax.axis('off')\n",
    "                image.savefig(\"temp\")\n",
    "                img = Image.open(\"temp.png\").convert(\"RGB\")\n",
    "                img = preprocess(img)\n",
    "                img.save(f\"TFR-CWT Representations/{shortname}-{i}-{channel}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files\n",
    "files = os.listdir(\"PreprocessedTFR\")\n",
    "\n",
    "# Prepare rows for the DataFrame\n",
    "rows = []\n",
    "\n",
    "labels = pd.DataFrame(columns=['file', 'label', 'aura', 'old', 'female'])\n",
    "channel_list = ['Fp1','AFp1','AF7','AF3','AFF5h','AFF1h','F9','F7','F5','F3','F1','FFT9h','FFT7h','FFC5h','FFC3h','FFC1h','FT9','FT7','FC5','FC3','FC1','FTT9h','FTT7h','FCC5h','FCC3h','FCC1h','T7','C5','C3','C1','TTP7h','CCP5h','CCP3h','CCP1h','TP9','TP7','CP5','CP3','CP1','CPz','TPP7h','CPP5h','CPP3h','CPP1h','P9','P7','P5','P3','P1','Pz','PPO9h','PPO5h','PPO1h','PO7','PO3','POz','PO9','POO9h','O1','POO1','I1','OI1h','Oz','Iz','Fpz','Fp2','AFp2','AFz','AF4','AF8','AFF2h','AFF6h','Fz','F2','F4','F6','F8','F10','FFC2h','FFC4h','FFC6h','FFT8h','FFT10h','FCz','FC2','FC4','FC6','FT8','FT10','FCC2h','FCC4h','FCC6h','FTT8h','FTT10h','Cz','C2','C4','C6','T8','CCP2h','CCP4h','CCP6h','TTP8h','CP2','CP4','CP6','TP8','TP10','CPP2h','CPP4h','CPP6h','TPP8h','P2','P4','P6','P8','P10','PPO2h','PPO6h','PPO10h','PO4','PO8','PO10','POO2','O2','POO10h','OI2h','I2','M1','M2','LO1','LO2','IO1','SO1','IO2','ECG','GSR1','GSR2','Erg1','Erg2','Resp','Plet','Temp','Status']\n",
    "aura = ['M1', 'M2', 'M4', 'M5', 'M6', 'M7', 'M10', 'M11', 'M14', 'M16', 'M17', 'M18']\n",
    "old = ['M6', 'M7', 'M8', 'M9', 'M14', 'M17', 'M18', 'C5', 'C7', 'C8', 'C9', 'C10', 'C17', 'C21']\n",
    "female = ['M2', 'M3', 'M5', 'M6', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16', 'M17', 'M18', 'C1', 'C3', 'C4', 'C8', 'C9', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21'] \n",
    "\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    shortname = file[:-9]\n",
    "    power = mne.time_frequency.read_tfrs(f'PreprocessedTFR/{file}')\n",
    "    num_epochs = power.shape[0]\n",
    "\n",
    "    is_old = shortname in old\n",
    "    is_female = shortname in female\n",
    "    is_aura = shortname in aura\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        for channel in channel_list:\n",
    "            label = [f'{shortname}-{i}-{channel}.png']\n",
    "            if shortname[0] == 'C':\n",
    "                label += [0, 0, int(is_old), int(is_female)]\n",
    "            elif shortname[0] == 'M':\n",
    "                label += [1, int(is_aura), int(is_old), int(is_female)]\n",
    "            rows.append(label)\n",
    "\n",
    "# Append all rows to the DataFrame at once\n",
    "labels = pd.concat([labels, pd.DataFrame(rows, columns=labels.columns)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.to_csv(\"labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.data_files, self.labels = self._load_data_files_and_labels(root_dir)\n",
    "\n",
    "    def _load_data_files_and_labels(self, root_dir):\n",
    "        data_files = []\n",
    "        labels = []\n",
    "        \n",
    "        for root, _, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.pt'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    data_files.append(file_path)\n",
    "                    label = 0 if file.startswith('C') else 1\n",
    "                    #label_float = np.float32(label)                   \n",
    "                    labels.append(label)\n",
    "        \n",
    "        return data_files, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data_files[idx]\n",
    "        tensor = torch.load(img_path)\n",
    "        #tensor = tensor.unsqueeze(0)  # Add an additional dimension\n",
    "        label = self.labels[idx]\n",
    "        return tensor, label\n",
    "    \n",
    "    \n",
    "train_dataset = AlexDataset('AlexNet/Train')\n",
    "valid_dataset = AlexDataset('AlexNet/Valid')\n",
    "test_dataset = AlexDataset('AlexNet/Test')\n",
    "\n",
    "from torch.utils.data import Sampler\n",
    "class SubsetRandomSampler(Sampler):\n",
    "    def __init__(self, data_source, subset_size):\n",
    "        self.data_source = data_source\n",
    "        self.subset_size = subset_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(np.random.choice(len(self.data_source), self.subset_size, replace=False))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.subset_size\n",
    "    \n",
    "# Create a custom sampler for the training set\n",
    "train_sampler = SubsetRandomSampler(train_dataset, subset_size=5000)\n",
    "valid_sampler = SubsetRandomSampler(valid_dataset, subset_size=5000)\n",
    "test_sampler = SubsetRandomSampler(test_dataset, subset_size=2500)\n",
    "\n",
    "# Create DataLoaders with the custom sampler\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, sampler=train_sampler, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=128, sampler=valid_sampler, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, sampler=test_sampler, shuffle=False, pin_memory=True)\n",
    "\n",
    "class CustomNeuralNetBinaryClassifier(NeuralNetClassifier):\n",
    "    def __init__(self, *args, train_loader=None, valid_loader=None, **kwargs):\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def get_iterator(self, dataset, training=False):\n",
    "        if training:\n",
    "            return self.train_loader\n",
    "        return self.valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CustomAlexNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 6 * 6, 4096),  # Update input dimension of first fc layer\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       34,944\n",
      "|    └─BatchNorm2d: 2-2                  192\n",
      "|    └─ReLU: 2-3                         --\n",
      "|    └─MaxPool2d: 2-4                    --\n",
      "├─Sequential: 1-2                        --\n",
      "|    └─Conv2d: 2-5                       614,656\n",
      "|    └─BatchNorm2d: 2-6                  512\n",
      "|    └─ReLU: 2-7                         --\n",
      "|    └─MaxPool2d: 2-8                    --\n",
      "├─Sequential: 1-3                        --\n",
      "|    └─Conv2d: 2-9                       885,120\n",
      "|    └─BatchNorm2d: 2-10                 768\n",
      "|    └─ReLU: 2-11                        --\n",
      "├─Sequential: 1-4                        --\n",
      "|    └─Conv2d: 2-12                      1,327,488\n",
      "|    └─BatchNorm2d: 2-13                 768\n",
      "|    └─ReLU: 2-14                        --\n",
      "├─Sequential: 1-5                        --\n",
      "|    └─Conv2d: 2-15                      884,992\n",
      "|    └─BatchNorm2d: 2-16                 512\n",
      "|    └─ReLU: 2-17                        --\n",
      "|    └─MaxPool2d: 2-18                   --\n",
      "├─Sequential: 1-6                        --\n",
      "|    └─Dropout: 2-19                     --\n",
      "|    └─Linear: 2-20                      37,752,832\n",
      "|    └─ReLU: 2-21                        --\n",
      "├─Sequential: 1-7                        --\n",
      "|    └─Dropout: 2-22                     --\n",
      "|    └─Linear: 2-23                      16,781,312\n",
      "|    └─ReLU: 2-24                        --\n",
      "├─Sequential: 1-8                        --\n",
      "|    └─Linear: 2-25                      8,194\n",
      "=================================================================\n",
      "Total params: 58,292,290\n",
      "Trainable params: 58,292,290\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Sequential: 1-1                        --\n",
       "|    └─Conv2d: 2-1                       34,944\n",
       "|    └─BatchNorm2d: 2-2                  192\n",
       "|    └─ReLU: 2-3                         --\n",
       "|    └─MaxPool2d: 2-4                    --\n",
       "├─Sequential: 1-2                        --\n",
       "|    └─Conv2d: 2-5                       614,656\n",
       "|    └─BatchNorm2d: 2-6                  512\n",
       "|    └─ReLU: 2-7                         --\n",
       "|    └─MaxPool2d: 2-8                    --\n",
       "├─Sequential: 1-3                        --\n",
       "|    └─Conv2d: 2-9                       885,120\n",
       "|    └─BatchNorm2d: 2-10                 768\n",
       "|    └─ReLU: 2-11                        --\n",
       "├─Sequential: 1-4                        --\n",
       "|    └─Conv2d: 2-12                      1,327,488\n",
       "|    └─BatchNorm2d: 2-13                 768\n",
       "|    └─ReLU: 2-14                        --\n",
       "├─Sequential: 1-5                        --\n",
       "|    └─Conv2d: 2-15                      884,992\n",
       "|    └─BatchNorm2d: 2-16                 512\n",
       "|    └─ReLU: 2-17                        --\n",
       "|    └─MaxPool2d: 2-18                   --\n",
       "├─Sequential: 1-6                        --\n",
       "|    └─Dropout: 2-19                     --\n",
       "|    └─Linear: 2-20                      37,752,832\n",
       "|    └─ReLU: 2-21                        --\n",
       "├─Sequential: 1-7                        --\n",
       "|    └─Dropout: 2-22                     --\n",
       "|    └─Linear: 2-23                      16,781,312\n",
       "|    └─ReLU: 2-24                        --\n",
       "├─Sequential: 1-8                        --\n",
       "|    └─Linear: 2-25                      8,194\n",
       "=================================================================\n",
       "Total params: 58,292,290\n",
       "Trainable params: 58,292,290\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "device = 'cuda'\n",
    "\n",
    "num_classes = 2\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n",
    "\n",
    "custommodel = CustomAlexNet(num_classes).to('cuda')\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(custommodel.parameters(), momentum=0.9, lr=0.1, weight_decay=0.0005)  \n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "summary(custommodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [40/40], Loss: 0.6890\n",
      "Accuracy of the network on the validation images: 42.94 %, Validation F1-score: 0.2580\n",
      "Epoch [2/10], Step [40/40], Loss: 0.6760\n",
      "Accuracy of the network on the validation images: 57.08 %, Validation F1-score: 0.4148\n",
      "Epoch [3/10], Step [40/40], Loss: 0.6926\n",
      "Accuracy of the network on the validation images: 41.90 %, Validation F1-score: 0.2474\n",
      "Epoch [4/10], Step [40/40], Loss: 0.6938\n",
      "Accuracy of the network on the validation images: 42.80 %, Validation F1-score: 0.2566\n",
      "Epoch [5/10], Step [40/40], Loss: 0.6946\n",
      "Accuracy of the network on the validation images: 42.40 %, Validation F1-score: 0.2525\n",
      "Epoch [6/10], Step [40/40], Loss: 0.7377\n",
      "Accuracy of the network on the validation images: 56.04 %, Validation F1-score: 0.4025\n",
      "Epoch [7/10], Step [40/40], Loss: 0.6892\n",
      "Accuracy of the network on the validation images: 57.12 %, Validation F1-score: 0.4153\n",
      "Epoch [8/10], Step [40/40], Loss: 0.6937\n",
      "Accuracy of the network on the validation images: 43.20 %, Validation F1-score: 0.2606\n",
      "Epoch [9/10], Step [40/40], Loss: 0.6820\n",
      "Accuracy of the network on the validation images: 56.78 %, Validation F1-score: 0.4113\n",
      "Epoch [10/10], Step [40/40], Loss: 0.6998\n",
      "Accuracy of the network on the validation images: 55.94 %, Validation F1-score: 0.4013\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = custommodel(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "    # Validation\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = custommodel(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            del images, labels, outputs\n",
    "            \n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='weighted')          \n",
    "            \n",
    "    \n",
    "        print('Accuracy of the network on the validation images: {:.2f} %, Validation F1-score: {:.4f}'.format(val_accuracy, val_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 55.04 %, Test F1-score: 0.3908\n",
      "Confusion Matrix:\n",
      " [[1376    0]\n",
      " [1124    0]]\n"
     ]
    }
   ],
   "source": [
    "all_labels = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = custommodel(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        del images, labels\n",
    "        \n",
    "        \n",
    "        \n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_f1 = f1_score(all_labels, all_preds, average='weighted')  \n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    print('Accuracy of the network on the test images: {:.2f} %, Test F1-score: {:.4f}'.format(test_accuracy, test_f1))\n",
    "    print('Confusion Matrix:\\n', conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8fa40a8dd040bb915ee991fa3f6ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train f1-score    train_acc    train_loss    valid f1-score    valid_acc    valid_loss      dur\n",
      "-------  ----------------  -----------  ------------  ----------------  -----------  ------------  -------\n",
      "      1            \u001b[36m0.0846\u001b[0m       \u001b[32m0.5152\u001b[0m        \u001b[35m0.6940\u001b[0m            \u001b[31m0.0000\u001b[0m       \u001b[94m0.5714\u001b[0m        \u001b[36m0.6884\u001b[0m  66.0885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a08594b27cc404cb0cab337580e4e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5334 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<class '__main__.CustomNeuralNetBinaryClassifier'>[initialized](\n",
       "  module_=AlexNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout(p=0.5, inplace=False)\n",
       "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "    )\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet = models.alexnet()\n",
    "alexnet.classifier[6] = nn.Linear(4096, 2)  # Change output layer to have 2 outputs (control/migraine)\n",
    "alexnet.cuda()\n",
    "\n",
    "n_epochs = 50\n",
    "anet = CustomNeuralNetBinaryClassifier(\n",
    "    alexnet,\n",
    "    train_split = predefined_split(valid_dataset),\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    max_epochs=n_epochs,\n",
    "    optimizer__lr=0.01,\n",
    "    optimizer__weight_decay = 0.1,\n",
    "    train_loader = train_loader,\n",
    "    valid_loader = valid_loader,\n",
    "    iterator_train__shuffle=False,\n",
    "    iterator_train__batch_size = 15,\n",
    "    device='cuda',\n",
    "    batch_size=15,\n",
    "    callbacks=[\n",
    "        (EpochScoring(scoring='accuracy', name='train_acc', on_train=True, lower_is_better=False)),\n",
    "        (EpochScoring(scoring='f1', name='train f1-score', on_train=True, lower_is_better=False)),\n",
    "        (EpochScoring(scoring='f1', name='valid f1-score', on_train=False, lower_is_better=False)),\n",
    "        #(\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "        #('EarlyStopping', EarlyStopping(patience=25, load_best=True)),\n",
    "        ('ProgressBar', ProgressBar())\n",
    "    ],\n",
    "    classes=[0,1]\n",
    "    )\n",
    "\n",
    "anet.fit(train_dataset, y=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(loader, model):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in loader:\n",
    "            inputs = inputs.cuda()\n",
    "            outputs = model(inputs)\n",
    "            prob = F.sigmoid(outputs)  # Apply sigmoid activation\n",
    "            y_pred.append(prob.cpu().numpy())\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "anet.module_.eval()\n",
    "\n",
    "# Access ground truth labels from test_dataset\n",
    "test_labels = np.array(test_dataset.labels)\n",
    "\n",
    "# Predict probabilities\n",
    "#y_pred = predict_proba(test_loader, anet.module_)\n",
    "y_pred = anet.predict(test_loader)\n",
    "\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "print(classification_report(test_labels, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEGNet Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EEGNET Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code chunk creates numpy arrays for training, validation, and test data. Note that i manually moved each of the epoch files into the respective folders\n",
    "# EEGnet/Train contains C1-C13 and M1-12; Valid contains C14-C17 and M13, M14 and M16; Test contains C18-C21 and M15, M17 and M18\n",
    "# The switch in the migraineur files has been made to make sure that both validation and test have at least 1 Aura-migraineur\n",
    "\n",
    "# Reading the first file in each folder\n",
    "train_epoch = mne.read_epochs(f\"EEGNet/Train/C1-epo.fif\") \n",
    "valid_epoch = mne.read_epochs(f\"EEGNet/Valid/C14-epo.fif\")\n",
    "test_epoch = mne.read_epochs(f\"EEGNet/Test/C18-epo.fif\")\n",
    "\n",
    "# Turning the epoch data into numpy array (this is done outside the loop to ensure correct dimensions, making appending easier)\n",
    "train_file = train_epoch.get_data()*1000\n",
    "valid_file = valid_epoch.get_data()*1000\n",
    "test_file =  test_epoch.get_data()*1000\n",
    "\n",
    "labels = [0] * train_file.shape[0] # since the first file for each of the loops is a control patient (C1, C14 and C18), I add a zero for each epoch of the file, since they are excluded in the loop. The same is done for validation and test further down\n",
    "\n",
    "# This loop reads each file in the EEGNet/Train folder except the first, turns the epoched data into numpy data and adds it to the initially created numpy files. Additionally, labels are created and added to a label file\n",
    "for file in os.listdir(\"EEGNet/Train\")[1:]:\n",
    "    epochs = mne.read_epochs(f\"EEGNet/Train/{file}\")\n",
    "    temp = epochs.get_data()*1000\n",
    "    train_file = np.append(train_file, temp, axis=0)\n",
    "    \n",
    "    label = 0 if file.startswith('C') else 1\n",
    "    labels.extend([label] * temp.shape[0])\n",
    "\n",
    "train_labels = np.array(labels)\n",
    "np.save(\"EEGNet/training_labels\", train_labels) # Saving the labels as their own file\n",
    "labels = [0] * valid_file.shape[0]\n",
    "    \n",
    "for file in os.listdir(\"EEGNet/Valid\")[1:]:\n",
    "    epochs = mne.read_epochs(f\"EEGNet/Valid/{file}\")\n",
    "    temp = epochs.get_data()*1000\n",
    "    valid_file = np.append(valid_file, temp, axis=0)\n",
    "    \n",
    "    label = 0 if file.startswith('C') else 1\n",
    "    labels.extend([label] * temp.shape[0])\n",
    "    \n",
    "valid_labels = np.array(labels)\n",
    "np.save(\"EEGNet/valid_labels\", valid_labels)\n",
    "labels = [0] * test_file.shape[0]\n",
    "\n",
    "for file in os.listdir(\"EEGNet/Test\")[1:]:\n",
    "    epochs = mne.read_epochs(f\"EEGNet/Test/{file}\")\n",
    "    temp = epochs.get_data()*1000\n",
    "    test_file = np.append(test_file, temp, axis=0)\n",
    "    \n",
    "    label = 0 if file.startswith('C') else 1\n",
    "    labels.extend([label] * temp.shape[0])    \n",
    "\n",
    "test_labels = np.array(labels)\n",
    "np.save(\"EEGNet/test_labels\", test_labels)\n",
    "\n",
    "#Ensuring that each epoch / trial has an associated label\n",
    "print(f'Training data shapes: {train_file.shape}, {valid_file.shape}, {test_file.shape}')\n",
    "print(f'Labels shapes: {train_labels.shape}, {valid_labels.shape}, {test_labels.shape}')\n",
    "\n",
    "np.save(\"EEGNet/train.npy\", train_file)\n",
    "np.save(\"EEGNet/valid.npy\", valid_file)\n",
    "np.save(\"EEGNet/test.npy\", test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code brings the code into the expected format for the used Pytorch implementation of EEGNet (--> Trials, Model-Channels, EEG-Channels, Timepoints)\n",
    "X_train      = np.load(\"EEGNet/train.npy\")\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_train = np.transpose(X_train, (0, 3, 1, 2))\n",
    "Y_train      = np.load(\"EEGNet/training_labels.npy\")\n",
    "\n",
    "X_valid   = np.load(\"EEGNet/valid.npy\")\n",
    "X_valid = np.expand_dims(X_valid, axis=-1)\n",
    "X_valid = np.transpose(X_valid, (0, 3, 1, 2))\n",
    "Y_valid   = np.load(\"EEGNet/valid_labels.npy\")\n",
    "\n",
    "X_test       = np.load(\"EEGNet/test.npy\")\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "X_test = np.transpose(X_test, (0, 3, 1, 2))\n",
    "Y_test       = np.load(\"EEGNet/test_labels.npy\")\n",
    "\n",
    "# convert labels to one-hot encodings.\n",
    "Y_train      = to_categorical(Y_train)\n",
    "Y_valid   = to_categorical(Y_valid)\n",
    "Y_test       = to_categorical(Y_test)\n",
    "   \n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_xtrain = torch.Tensor(X_train)\n",
    "tensor_ytrain = torch.Tensor(Y_train)\n",
    "tensor_xvalid = torch.Tensor(X_valid)\n",
    "tensor_yvalid = torch.Tensor(Y_valid)\n",
    "tensor_xtest = torch.Tensor(X_test)\n",
    "tensor_ytest = torch.Tensor(Y_test)\n",
    "\n",
    "tensor_xtrain = tensor_xtrain.float()\n",
    "tensor_xvalid = tensor_xvalid.float()\n",
    "tensor_xtest = tensor_xtest.float()\n",
    "\n",
    "# Calculate mean and standard deviation along the trials/EEG-epochs axis (axis=0)\n",
    "mean = torch.mean(tensor_xtrain, dim=(0, 2, 3), keepdim=True)\n",
    "std = torch.std(tensor_xtrain, dim=(0, 2, 3), keepdim=True)\n",
    "\n",
    "vmean = torch.mean(tensor_xvalid, dim=(0, 2, 3), keepdim=True)\n",
    "vstd = torch.std(tensor_xvalid, dim=(0, 2, 3), keepdim=True)\n",
    "\n",
    "tmean = torch.mean(tensor_xtest, dim=(0, 2, 3), keepdim=True)\n",
    "tstd = torch.std(tensor_xtest, dim=(0, 2, 3), keepdim=True)\n",
    "\n",
    "# Standardize the data\n",
    "standardized_tensor_xtrain = (tensor_xtrain - mean) / std\n",
    "standardized_tensor_xvalid = (tensor_xvalid - vmean) / vstd\n",
    "standardized_tensor_xtest = (tensor_xtest - tmean) / tstd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(standardized_tensor_xtrain, tensor_ytrain)\n",
    "#train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "torch.save(train_dataset, \"EEGNet/train_dataset_normalized.pt\")\n",
    "\n",
    "valid_dataset = TensorDataset(standardized_tensor_xvalid, tensor_yvalid)\n",
    "#valid_loader =  DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "torch.save(valid_dataset, \"EEGNet/valid_dataset_normalized.pt\")\n",
    "\n",
    "test_dataset = TensorDataset(standardized_tensor_xtest, tensor_ytest)\n",
    "#test_loader =  DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "torch.save(test_dataset, \"EEGNet/test_dataset_normalized.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tensor_xtrain = torch.Tensor(X_train)\n",
    "\n",
    "tensor_ytrain = torch.Tensor(Y_train)\n",
    "tensor_xvalid = torch.Tensor(X_valid)\n",
    "tensor_yvalid = torch.Tensor(Y_valid)\n",
    "tensor_xtest = torch.Tensor(X_test)\n",
    "tensor_ytest = torch.Tensor(Y_test)\n",
    "\n",
    "train_dataset = TensorDataset(tensor_xtrain, tensor_ytrain)\n",
    "#train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "torch.save(train_dataset, \"EEGNet/train_dataset_normalized.pt\")\n",
    "\n",
    "valid_dataset = TensorDataset(tensor_xvalid, tensor_yvalid)\n",
    "#valid_loader =  DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "torch.save(valid_dataset, \"EEGNet/valid_dataset_normalized.pt\")\n",
    "\n",
    "test_dataset = TensorDataset(tensor_xtest, tensor_ytest)\n",
    "#test_loader =  DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "torch.save(test_dataset, \"EEGNet/test_dataset_normalized.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code turns each file into tensors, which are then used to create datasets and dataloaders for the models\n",
    "tensor_xtrain = torch.Tensor(X_train)\n",
    "tensor_ytrain = torch.Tensor(Y_train)\n",
    "tensor_xvalid = torch.Tensor(X_valid)\n",
    "tensor_yvalid = torch.Tensor(Y_valid)\n",
    "tensor_xtest = torch.Tensor(X_test)\n",
    "tensor_ytest = torch.Tensor(Y_test)\n",
    "\n",
    "train_dataset = TensorDataset(tensor_xtrain, tensor_ytrain)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "torch.save(train_dataset, \"EEGNet/train_dataset.pt\")\n",
    "torch.save(train_loader, \"EEGNet/train_loader.pt\")\n",
    "\n",
    "valid_dataset = TensorDataset(tensor_xvalid, tensor_yvalid)\n",
    "valid_loader =  DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "torch.save(valid_dataset, \"EEGNet/valid_dataset.pt\")\n",
    "torch.save(valid_loader, \"EEGNet/valid_loader.pt\")\n",
    "\n",
    "test_dataset = TensorDataset(tensor_xtest, tensor_ytest)\n",
    "test_loader =  DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "torch.save(test_dataset, \"EEGNet/test_dataset.pt\")\n",
    "torch.save(test_loader, \"EEGNet/test_loader.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EEGNet via TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'EEGNet/training_labels.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m X_train      \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEEGNet/train.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(X_train, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m Y_train      \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEEGNet/training_labels.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m X_valid   \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEEGNet/valid.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m X_valid \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(X_valid, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rieve\\Documents\\MSc Data Science and Society\\Block 3\\ThesisLocal\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'EEGNet/training_labels.npy'"
     ]
    }
   ],
   "source": [
    "X_train      = np.load(\"EEGNet/train.npy\")\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "Y_train      = np.load(\"EEGNet/training_labels.npy\")\n",
    "\n",
    "X_valid   = np.load(\"EEGNet/valid.npy\")\n",
    "X_valid = np.expand_dims(X_valid, axis=-1)\n",
    "Y_valid   = np.load(\"EEGNet/valid_labels.npy\")\n",
    "\n",
    "X_test       = np.load(\"EEGNet/test.npy\")\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "Y_test       = np.load(\"EEGNet/test_labels.npy\")\n",
    "\n",
    "kernels, chans, samples = 1, 144, 1024\n",
    "\n",
    "# convert labels to one-hot encodings.\n",
    "Y_train      = to_categorical(Y_train)\n",
    "Y_valid      = to_categorical(Y_valid)\n",
    "Y_test       = to_categorical(Y_test)\n",
    "   \n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
    "# model configurations may do better, but this is a good starting point)\n",
    "model = EEGNet(nb_classes = 2, Chans = chans, Samples = samples, \n",
    "               dropoutRate = 0.5, kernLength = 32, F1 = 8, D = 2, F2 = 16, \n",
    "               dropoutType = 'Dropout')\n",
    "\n",
    "# compile the model and set the optimizers\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# count number of parameters in the model\n",
    "#numParams    = model.count_params()    \n",
    "\n",
    "# set a valid path for your system to record model checkpoints\n",
    "checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.keras', verbose=1,\n",
    "                               save_best_only=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedModel = model.fit(X_train, Y_train, batch_size = 16, epochs = 11, \n",
    "                        verbose = 1, validation_data=(X_valid, Y_valid),\n",
    "                        callbacks=[checkpointer])\n",
    "\n",
    "# load optimal weights\n",
    "model.load_weights('/tmp/checkpoint.keras')\n",
    "\n",
    "#Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    " \n",
    "#Save the model\n",
    "model.save('eegnet_tf.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EEGNet in Pytorch (via https://github.com/Amir-Hofo/EEGNet_Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup version with Sigmoid function\n",
    "class PytorchEEG(nn.Module): \n",
    "    def __init__(self, nb_classes=2, nb_channels=144, nb_samples=1024, F1=8, D=2, freq=512, dropout=0.25, device='cuda'):\n",
    "        super().__init__()\n",
    "        #Parameters\n",
    "        self.nb_samples = nb_samples\n",
    "        self.nb_classes= nb_classes\n",
    "        self.nb_channels = nb_channels\n",
    "        self.nb_inputs = 1\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F1*D\n",
    "        self.freq = freq\n",
    "        self.dropout = dropout\n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        self.kernel_size_1 = (1,round(self.freq/2)) \n",
    "        self.kernel_size_2 = (nb_channels, 1)\n",
    "        self.kernel_size_3 = (1, round(self.freq/8))\n",
    "        self.kernel_size_4 = (1, 1)\n",
    "        \n",
    "        self.kernel_avgpool_1 = (1,4)\n",
    "        self.kernel_avgpool_2= (1,8)\n",
    "        \n",
    "        self.kernel_padding_1 = (int(round((self.kernel_size_1[0]-1)/2)) , (int(round((self.kernel_size_1[1]-1)/2)))-1)\n",
    "        self.kernel_padding_3 = (int(round((self.kernel_size_3[0]-1)/2)) , (int(round((self.kernel_size_3[1]-1)/2))))\n",
    "        \n",
    "        # layer 1\n",
    "        self.conv2d = nn.Conv2d(self.nb_inputs, self.F1, self.kernel_size_1, padding=self.kernel_padding_1)\n",
    "        self.Batch_normalization_1 = nn.BatchNorm2d(self.F1)\n",
    "        # layer 2\n",
    "        self.Depthwise_conv2D = nn.Conv2d(self.F1, self.D*self.F1, self.kernel_size_2, groups= self.F1)\n",
    "        self.Batch_normalization_2 = nn.BatchNorm2d(self.D*self.F1)\n",
    "        self.Elu = nn.ELU()\n",
    "        self.Average_pooling2D_1 = nn.AvgPool2d(self.kernel_avgpool_1)\n",
    "        self.Dropout = nn.Dropout2d(self.dropout)\n",
    "        # layer 3\n",
    "        self.Separable_conv2D_depth = nn.Conv2d(self.D*self.F1, self.D*self.F1, self.kernel_size_3,\n",
    "                                                padding=self.kernel_padding_3, groups= self.D*self.F1)\n",
    "        self.Separable_conv2D_point = nn.Conv2d(self.D*self.F1, self.F2, self.kernel_size_4)\n",
    "        self.Batch_normalization_3 = nn.BatchNorm2d(self.F2)\n",
    "        self.Average_pooling2D_2 = nn.AvgPool2d(self.kernel_avgpool_2)\n",
    "        # layer 4\n",
    "        self.Flatten = nn.Flatten()\n",
    "        self.Dense = nn.Linear(self.F2*round(self.nb_samples/32), self.nb_classes)\n",
    "        \n",
    "        if self.nb_classes == 2:\n",
    "            self.Sigmoid = nn.Sigmoid()\n",
    "        else:\n",
    "            self.Softmax = nn.Softmax(dim= 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # layer 1\n",
    "        y = self.Batch_normalization_1(self.conv2d(x)) #.relu()\n",
    "        # layer 2\n",
    "        y = self.Batch_normalization_2(self.Depthwise_conv2D(y))\n",
    "        y = self.Elu(y)\n",
    "        y = self.Dropout(self.Average_pooling2D_1(y))\n",
    "        # layer 3\n",
    "        y = self.Separable_conv2D_depth(y)\n",
    "        y = self.Batch_normalization_3(self.Separable_conv2D_point(y))\n",
    "        y = self.Elu(y)\n",
    "        y = self.Dropout(self.Average_pooling2D_2(y))\n",
    "        # layer 4\n",
    "        y = self.Flatten(y)\n",
    "        y = self.Dense(y)\n",
    "        \n",
    "        if self.nb_classes == 2:\n",
    "            y = self.Sigmoid(y)\n",
    "        else:\n",
    "            y = self.Softmax(y)\n",
    "        return y\n",
    "    \n",
    "    def max_norm(self):\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters():\n",
    "                if len(param.shape) > 1:\n",
    "                    param.clamp_(-1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PytorchEEG(\n",
       "  (conv2d): Conv2d(1, 64, kernel_size=(1, 256), stride=(1, 1), padding=(0, 127))\n",
       "  (Batch_normalization_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (Depthwise_conv2D): Conv2d(64, 128, kernel_size=(143, 1), stride=(1, 1), groups=64)\n",
       "  (Batch_normalization_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (Elu): ELU(alpha=1.0)\n",
       "  (Average_pooling2D_1): AvgPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0)\n",
       "  (Dropout): Dropout2d(p=0.4, inplace=False)\n",
       "  (Separable_conv2D_depth): Conv2d(128, 128, kernel_size=(1, 64), stride=(1, 1), padding=(0, 32), groups=128)\n",
       "  (Separable_conv2D_point): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (Batch_normalization_3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (Average_pooling2D_2): AvgPool2d(kernel_size=(1, 8), stride=(1, 8), padding=0)\n",
       "  (Flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (Dense): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device= 'cuda'\n",
    "class PytorchEEG(nn.Module): \n",
    "    def __init__(self, nb_classes=2, nb_channels=144, nb_samples=1024, F1=8, D=2, freq=512, dropout=0.25, device='cuda'):\n",
    "        super().__init__()\n",
    "        #Parameters\n",
    "        self.nb_samples = nb_samples\n",
    "        self.nb_classes= nb_classes\n",
    "        self.nb_channels = nb_channels\n",
    "        self.nb_inputs = 1\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F1*D\n",
    "        self.freq = freq\n",
    "        self.dropout = dropout\n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        self.kernel_size_1 = (1,round(self.freq/2)) \n",
    "        self.kernel_size_2 = (nb_channels, 1)\n",
    "        self.kernel_size_3 = (1, round(self.freq/8))\n",
    "        self.kernel_size_4 = (1, 1)\n",
    "        \n",
    "        self.kernel_avgpool_1 = (1,4)\n",
    "        self.kernel_avgpool_2= (1,8)\n",
    "        \n",
    "        self.kernel_padding_1 = (int(round((self.kernel_size_1[0]-1)/2)) , (int(round((self.kernel_size_1[1]-1)/2)))-1)\n",
    "        self.kernel_padding_3 = (int(round((self.kernel_size_3[0]-1)/2)) , (int(round((self.kernel_size_3[1]-1)/2))))\n",
    "        \n",
    "        # layer 1\n",
    "        self.conv2d = nn.Conv2d(self.nb_inputs, self.F1, self.kernel_size_1, padding=self.kernel_padding_1)\n",
    "        self.Batch_normalization_1 = nn.BatchNorm2d(self.F1)\n",
    "        # layer 2\n",
    "        self.Depthwise_conv2D = nn.Conv2d(self.F1, self.D*self.F1, self.kernel_size_2, groups= self.F1)\n",
    "        self.Batch_normalization_2 = nn.BatchNorm2d(self.D*self.F1)\n",
    "        self.Elu = nn.ELU()\n",
    "        self.Average_pooling2D_1 = nn.AvgPool2d(self.kernel_avgpool_1)\n",
    "        self.Dropout = nn.Dropout2d(self.dropout)\n",
    "        # layer 3\n",
    "        self.Separable_conv2D_depth = nn.Conv2d(self.D*self.F1, self.D*self.F1, self.kernel_size_3,\n",
    "                                                padding=self.kernel_padding_3, groups= self.D*self.F1)\n",
    "        self.Separable_conv2D_point = nn.Conv2d(self.D*self.F1, self.F2, self.kernel_size_4)\n",
    "        self.Batch_normalization_3 = nn.BatchNorm2d(self.F2)\n",
    "        self.Average_pooling2D_2 = nn.AvgPool2d(self.kernel_avgpool_2)\n",
    "        # layer 4\n",
    "        self.Flatten = nn.Flatten()\n",
    "        self.Dense = nn.Linear(self.F2*round(self.nb_samples/32), self.nb_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # layer 1\n",
    "        y = self.Batch_normalization_1(self.conv2d(x)) #.relu()\n",
    "        # layer 2\n",
    "        y = self.Batch_normalization_2(self.Depthwise_conv2D(y))\n",
    "        y = self.Elu(y)\n",
    "        y = self.Dropout(self.Average_pooling2D_1(y))\n",
    "        # layer 3\n",
    "        y = self.Separable_conv2D_depth(y)\n",
    "        y = self.Batch_normalization_3(self.Separable_conv2D_point(y))\n",
    "        y = self.Elu(y)\n",
    "        y = self.Dropout(self.Average_pooling2D_2(y))\n",
    "        # layer 4\n",
    "        y = self.Flatten(y)\n",
    "        y = self.Dense(y)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def max_norm(self):\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters():\n",
    "                if len(param.shape) > 1:\n",
    "                    param.clamp_(-1.0, 1.0)\n",
    "                    \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "model = PytorchEEG(nb_classes=2, nb_samples=512, nb_channels=143, F1 = 64, D = 2, dropout=0.4)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, valid_loader, criterion, optimizer):\n",
    "    model.train(True)\n",
    "    loss_train = AverageMeter()\n",
    "    acc_train = Accuracy(task='binary').to(device)\n",
    "    \n",
    "    loss_val = AverageMeter()\n",
    "    acc_val = Accuracy(task='binary').to(device)\n",
    "    \n",
    "    train_loader = tqdm(train_loader, desc=\"Training\", leave=False) #Gives progress bar\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        model.max_norm()\n",
    "\n",
    "        loss_train.update(loss.item())\n",
    "        acc_train(outputs, targets.int())\n",
    "        train_loader.set_postfix({'loss': loss_train.avg, 'accuracy': acc_train.compute().item()}, refresh=True)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (vinputs, vtargets) in enumerate(valid_loader):\n",
    "            vinputs = vinputs.to(device)\n",
    "            vtargets = vtargets.to(device)\n",
    "            \n",
    "            voutputs = model(vinputs)\n",
    "            vloss = criterion(voutputs, vtargets)\n",
    "            loss_val.update(vloss.item())\n",
    "            acc_val(voutputs, vtargets.int())\n",
    "    \n",
    "    return model, loss_train.avg, acc_train.compute().item(), loss_val.avg, acc_val.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('PreprocessedEpochData/X_train.npy')\n",
    "X_val = np.load('PreprocessedEpochData/X_val.npy')\n",
    "X_test = np.load('PreprocessedEpochData/X_test.npy')\n",
    "\n",
    "y_train = np.load('PreprocessedEpochData/y_train.npy')\n",
    "y_val = np.load('PreprocessedEpochData/y_val.npy')\n",
    "y_test = np.load('PreprocessedEpochData/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train and y_train are numpy arrays\n",
    "X_train_reshaped = np.expand_dims(X_train, axis=1)  # Add a new axis at index 1\n",
    "X_train_tensor = torch.tensor(X_train_reshaped, dtype=torch.float32)  # Convert numpy array to tensor\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_train_tensor = torch.tensor(y_train_cat, dtype=torch.float32)  # Convert numpy array to tensor\n",
    "\n",
    "X_val_reshaped = np.expand_dims(X_val, axis=1)  # Add a new axis at index 1\n",
    "X_val_tensor = torch.tensor(X_val_reshaped, dtype=torch.float32)  # Convert numpy array to tensor\n",
    "y_val_cat = to_categorical(y_val)\n",
    "y_val_tensor = torch.tensor(y_val_cat, dtype=torch.float32)  # Convert numpy array to tensor\n",
    "\n",
    "X_test_reshaped = np.expand_dims(X_test, axis=1)  # Add a new axis at index 1\n",
    "X_test_tensor = torch.tensor(X_test_reshaped, dtype=torch.float32)  # Convert numpy array to tensor\n",
    "y_test_cat = to_categorical(y_test)\n",
    "y_test_tensor = torch.tensor(y_test_cat, dtype=torch.float32)  # Convert numpy array to tensor\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_data = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = torch.load('EEGNet/Correct-data/train_data_bce_z.pt') #or 'EEGNet/Correct-data/train_dataset_BCE.pt' when working with BCELoss\n",
    "# val_data = torch.load('EEGNet/Correct-data/val_data_bce_z.pt')\n",
    "# test_data = torch.load('EEGNet/Correct-data/test_data_bce_z.pt') # or 'EEGNet/Correct-data/test_dataset_BCE.pt' when working with BCELoss\n",
    "\n",
    "# Create dataloaders for training and validation\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:\n",
      " Training Loss = 0.7147, Training Accuracy = 49%\n",
      " Validation Loss = 0.7868, Validation Accuracy = 49% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1:\n",
      " Training Loss = 0.7068, Training Accuracy = 50%\n",
      " Validation Loss = 0.7494, Validation Accuracy = 51% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2:\n",
      " Training Loss = 0.6892, Training Accuracy = 53%\n",
      " Validation Loss = 0.7283, Validation Accuracy = 56% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3:\n",
      " Training Loss = 0.6814, Training Accuracy = 54%\n",
      " Validation Loss = 0.769, Validation Accuracy = 57% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4:\n",
      " Training Loss = 0.6693, Training Accuracy = 56%\n",
      " Validation Loss = 0.6929, Validation Accuracy = 56% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5:\n",
      " Training Loss = 0.6631, Training Accuracy = 57%\n",
      " Validation Loss = 0.7833, Validation Accuracy = 55% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6:\n",
      " Training Loss = 0.6592, Training Accuracy = 58%\n",
      " Validation Loss = 0.7429, Validation Accuracy = 56% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7:\n",
      " Training Loss = 0.6556, Training Accuracy = 58%\n",
      " Validation Loss = 0.9093, Validation Accuracy = 56% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8:\n",
      " Training Loss = 0.6475, Training Accuracy = 59%\n",
      " Validation Loss = 0.9058, Validation Accuracy = 55% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9:\n",
      " Training Loss = 0.6465, Training Accuracy = 59%\n",
      " Validation Loss = 0.8002, Validation Accuracy = 53% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10:\n",
      " Training Loss = 0.6374, Training Accuracy = 61%\n",
      " Validation Loss = 0.9386, Validation Accuracy = 55% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11:\n",
      " Training Loss = 0.6381, Training Accuracy = 60%\n",
      " Validation Loss = 0.9186, Validation Accuracy = 54% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12:\n",
      " Training Loss = 0.6339, Training Accuracy = 61%\n",
      " Validation Loss = 0.9372, Validation Accuracy = 54% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13:\n",
      " Training Loss = 0.632, Training Accuracy = 61%\n",
      " Validation Loss = 0.7125, Validation Accuracy = 56% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14:\n",
      " Training Loss = 0.6239, Training Accuracy = 63%\n",
      " Validation Loss = 1.036, Validation Accuracy = 55% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15:\n",
      " Training Loss = 0.6249, Training Accuracy = 63%\n",
      " Validation Loss = 0.9536, Validation Accuracy = 52% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16:\n",
      " Training Loss = 0.6233, Training Accuracy = 62%\n",
      " Validation Loss = 1.204, Validation Accuracy = 57% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17:\n",
      " Training Loss = 0.6166, Training Accuracy = 63%\n",
      " Validation Loss = 1.095, Validation Accuracy = 55% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18:\n",
      " Training Loss = 0.6097, Training Accuracy = 64%\n",
      " Validation Loss = 1.103, Validation Accuracy = 54% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "Adamoptimizer = optim.Adam(model.parameters(), lr= 0.00001, weight_decay=1e-6)\n",
    "Adamax = optim.Adamax(model.parameters(), lr= 0.000006)\n",
    "NAdamoptimizer = optim.NAdam(model.parameters(), lr= 0.01)\n",
    "RMSoptimizer= optim.RMSprop(model.parameters(), lr= 0.00001, weight_decay=1e-6)\n",
    "SGDoptimizer = optim.SGD(model.parameters(), lr= 0.001, momentum=0.9)\n",
    "\n",
    "# Data collection\n",
    "loss_train_hist = []\n",
    "acc_train_hist = []\n",
    "loss_val_hist = []\n",
    "acc_val_hist = []\n",
    "\n",
    "for epoch in range(301): #choose number of epochs in range\n",
    "    model, loss_train, acc_train, loss_val, acc_val = train_one_epoch(model, train_loader, val_loader, criterion, optimizer=Adamax)\n",
    "  \n",
    "    loss_train_hist.append(loss_train)\n",
    "    acc_train_hist.append(acc_train)\n",
    "\n",
    "    loss_val_hist.append(loss_val)\n",
    "    acc_val_hist.append(acc_val)    \n",
    "\n",
    "    print(f'epoch {epoch}:')\n",
    "    print(f' Training Loss = {loss_train:.4}, Training Accuracy = {int(acc_train*100)}%')\n",
    "    print(f' Validation Loss = {loss_val:.4}, Validation Accuracy = {int(acc_val*100)}% \\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = Accuracy(task=\"binary\").to('cuda')\n",
    "all_pred = []\n",
    "all_labels = []\n",
    "all_losses = []\n",
    "for testdata, labels in test_loader:\n",
    "    testdata = testdata.cuda()\n",
    "    labels = labels.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        y_pred = model(testdata)        \n",
    "        y_pred_binary = (y_pred > 0.5).float()\n",
    "        \n",
    "        all_pred.append(y_pred_binary)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_pred = torch.cat(all_pred)\n",
    "all_labels = torch.cat(all_labels)\n",
    "\n",
    "accuracy_score = accuracy(all_pred, all_labels)\n",
    "print(f'Overall Accuracy: {accuracy_score.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "figure, axis = plt.subplots(2, 2)\n",
    "# For Train Accuracy \n",
    "axis[0, 0].plot(range(48), acc_train_hist) \n",
    "axis[0, 0].set_title(\"Training accuracy\") \n",
    "  \n",
    "# For Cosine Function \n",
    "axis[0, 1].plot(range(48), acc_val_hist) \n",
    "axis[0, 1].set_title(\"Validation accuracy\") \n",
    "  \n",
    "# For Tangent Function \n",
    "axis[1, 0].plot(range(48), loss_train_hist) \n",
    "axis[1, 0].set_title(\"Training loss\") \n",
    "  \n",
    "# For Tanh Function \n",
    "axis[1, 1].plot(range(48), loss_val_hist) \n",
    "axis[1, 1].set_title(\"Validation loss\") \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(51), loss_val_hist, 'b-', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(51), acc_val_hist, 'b-', label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Acc')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set your model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize empty lists to store predictions\n",
    "all_predicted_probs = []\n",
    "all_true_labels = []\n",
    "\n",
    "# Iterate over batches in the test dataloader\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.cuda()\n",
    "    labels = labels.cuda()\n",
    "    # Forward pass: compute predicted probabilities\n",
    "    with torch.no_grad():\n",
    "        predicted_probs = model(inputs)\n",
    "    \n",
    "    # Append predicted probabilities and true labels to the lists\n",
    "    all_predicted_probs.append(predicted_probs)\n",
    "    all_true_labels.append(labels)\n",
    "\n",
    "# Concatenate predictions and true labels from all batches\n",
    "predicted_labels = torch.cat(all_predicted_probs, dim=0)\n",
    "true_labels = torch.cat(all_true_labels, dim=0)\n",
    "\n",
    "true_labels = true_labels.cpu().numpy()\n",
    "predicted_labels = predicted_labels.cpu().numpy()\n",
    "\n",
    "# Now you can calculate metrics using all_predicted_probs and all_true_labels\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "auc_roc = roc_auc_score(true_labels, predicted_probs[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(true_labels, predicted_probs[:, 1])\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    F1 = trial.suggest_int('F1', 4, 16)\n",
    "    #D = trial.suggest_int('D', 2, 8)\n",
    "    #dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    lr = trial.suggest_loguniform('lr', 0.0005, 0.001)\n",
    "    \n",
    "    # Initialize model with suggested hyperparameters\n",
    "    model = PytorchEEG(nb_classes=2, nb_samples=1024, nb_channels=141, F1=F1, D=2, dropout=0.5)\n",
    "    model.cuda()\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    loss_fn = nn.BCELoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n",
    "    \n",
    "    # Training loop for a few epochs (reduce number of epochs for faster optimization)\n",
    "    na = \"NA\"\n",
    "    print(f\"Now: F1:{F1}, D:{na}, dropout:{na}, lr:{lr}\")\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model, loss_train, acc_train, loss_val, acc_val = train_one_epoch(model, train_loader, val_loader, loss_fn, optimizer)\n",
    "        print(f'epoch {epoch}:')\n",
    "        print(f' Training Loss = {loss_train:.4}, Training Accuracy = {int(acc_train*100)}%')\n",
    "        print(f' Validation Loss = {loss_val:.4}, Validation Accuracy = {int(acc_val*100)}% \\n') \n",
    "    # Return the validation loss for Optuna to minimize\n",
    "    return loss_val\n",
    "\n",
    "# Create an Optuna study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "best_loss = study.best_value\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "print(\"Best Validation Loss:\", best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EEGNet via torchEEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load('EEGNet/Correct-data/train_data.pt')\n",
    "val_data = torch.load('EEGNet/Correct-data/val_data.pt')\n",
    "test_data = torch.load('EEGNet/Correct-data/test_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for training and validation\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torchEEG(chunk_size=1024, num_electrodes=141, num_classes=2, F1=64, D=2, F2=128, dropout=0.4)\n",
    "model2.cuda()\n",
    "\n",
    "trainer = ClassifierTrainer(model=model2,\n",
    "                            num_classes=2,\n",
    "                            lr=0.00001,\n",
    "                            weight_decay=1e-6,\n",
    "                            accelerator=\"gpu\",\n",
    "                            metrics=['accuracy', 'f1score'])\n",
    "\n",
    "\n",
    "trainer.fit(train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            max_epochs=300,\n",
    "            callbacks=[pl.callbacks.ModelCheckpoint(save_last=True)],\n",
    "            enable_progress_bar=True,\n",
    "            enable_model_summary=True,\n",
    "            limit_val_batches=0.0)\n",
    "\n",
    "score = trainer.test(test_loader,\n",
    "                     enable_progress_bar=True,\n",
    "                     enable_model_summary=True)[0]\n",
    "print(f'Test accuracy for F1=8, D=2, F2=16, dropout=0.3: {score[\"test_accuracy\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EEGNet via Braindecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store data and labels\n",
    "X_train = np.empty((0,143,512))  # EEG training data\n",
    "y_train = np.empty((0,))  # Training labels\n",
    "\n",
    "X_val = np.empty((0,143,512))\n",
    "y_val = np.empty((0,))\n",
    "\n",
    "X_test = np.empty((0,143,512))  # EEG testing data\n",
    "y_test = np.empty((0,))   # Testing labels\n",
    "\n",
    "\n",
    "# Loop through each subject in the training set\n",
    "for file in os.listdir(\"PreprocessedEpochData/Train\"):\n",
    "    subject_file = np.load(f'PreprocessedEpochData/Train/{file}')\n",
    "    print(f'Now {file} in train')\n",
    "    \n",
    "    # Determine label based on whether the subject is a control or migraineur\n",
    "    if file[0] == 'C':\n",
    "        label = 0  # Control\n",
    "    elif file[0] == 'M':\n",
    "        label = 1  # Migraineur\n",
    "\n",
    "    # Append subject data and labels to the overall training sets\n",
    "    X_train = np.append(X_train, subject_file, axis=0)\n",
    "    y_train = np.append(y_train, [label]*len(subject_file))\n",
    "\n",
    "# Loop through each subject in the training set\n",
    "for file in os.listdir(\"PreprocessedEpochData/Valid\"):\n",
    "    subject_file = np.load(f'PreprocessedEpochData/Valid/{file}')\n",
    "    print(f'Now {file} in validation')\n",
    "    \n",
    "    # Determine label based on whether the subject is a control or migraineur\n",
    "    if file[0] == 'C':\n",
    "        label = 0  # Control\n",
    "    elif file[0] == 'M':\n",
    "        label = 1  # Migraineur\n",
    "\n",
    "    # Append subject data and labels to the overall training sets\n",
    "    X_val = np.append(X_val, subject_file, axis=0)\n",
    "    y_val = np.append(y_val, [label]*len(subject_file))\n",
    "\n",
    "# Loop through each subject in the testing set\n",
    "for file in os.listdir(\"PreprocessedEpochData/Test\"):\n",
    "    subject_file = np.load(f'PreprocessedEpochData/Test/{file}')\n",
    "    print(f'Now {file} in test')\n",
    "    \n",
    "    # Determine label based on whether the subject is a control or migraineur\n",
    "    if file[0] == 'C':\n",
    "        label = 0  # Control\n",
    "    else:\n",
    "        label = 1  # Migraineur\n",
    "\n",
    "    # Append subject data and labels to the overall testing sets\n",
    "    X_test = np.append(X_test, subject_file, axis=0)\n",
    "    y_test = np.append(y_test, [label]*len(subject_file))\n",
    "\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_val = y_val.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "np.save('PreprocessedEpochData/X_train.npy', X_train)\n",
    "np.save('PreprocessedEpochData/X_val.npy', X_val)\n",
    "np.save('PreprocessedEpochData/X_test.npy', X_test)\n",
    "np.save('PreprocessedEpochData/y_train.npy', y_train)\n",
    "np.save('PreprocessedEpochData/y_val.npy', y_val)\n",
    "np.save('PreprocessedEpochData/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#! (VERY IMPORTANT STEP)\n",
    "#Standardizing files in training, validation and test set \n",
    "def standardizete(f):\n",
    "    epoch = mne.read_epochs(f'EEGNet/Test/{f}')\n",
    "    epochdata = epoch.pick(['eeg'])\n",
    "    a = epochdata.get_data()\n",
    "    labels = a.shape[0]\n",
    "    scale = mne.decoding.Scaler(info=epochdata.info, scalings='mean')\n",
    "    if f[0] == 'C':\n",
    "        epo_std = scale.fit_transform(a, y=np.zeros(labels, dtype=np.int32))\n",
    "    elif f[0] == 'M':\n",
    "        epo_std = scale.fit_transform(a, y=np.ones(labels, dtype=np.int32))\n",
    "    return epo_std\n",
    "\n",
    "def standardizetr(f):\n",
    "    epoch = mne.read_epochs(f'EEGNet/Train/{f}')\n",
    "    epochdata = epoch.pick(['eeg'])\n",
    "    a = epochdata.get_data()\n",
    "    labels = a.shape[0]\n",
    "    scale = mne.decoding.Scaler(info=epochdata.info, scalings='mean')\n",
    "    if f[0] == 'C':\n",
    "        epo_std = scale.fit_transform(a, y=np.zeros(labels, dtype=np.int32))\n",
    "    elif f[0] == 'M':\n",
    "        epo_std = scale.fit_transform(a, y=np.ones(labels, dtype=np.int32))\n",
    "    return epo_std\n",
    "\n",
    "def standardizeval(f):\n",
    "    epoch = mne.read_epochs(f'EEGNet/Valid/{f}')\n",
    "    epochdata = epoch.pick(['eeg'])\n",
    "    a = epochdata.get_data()\n",
    "    labels = a.shape[0]\n",
    "    scale = mne.decoding.Scaler(info=epochdata.info, scalings='mean')\n",
    "    if f[0] == 'C':\n",
    "        epo_std = scale.fit_transform(a, y=np.zeros(labels, dtype=np.int32))\n",
    "    elif f[0] == 'M':\n",
    "        epo_std = scale.fit_transform(a, y=np.ones(labels, dtype=np.int32))\n",
    "    return epo_std\n",
    "\n",
    "# Lists to store data and labels\n",
    "X_train = np.empty((0,141,1024))  # EEG training data\n",
    "y_train = np.empty((0,))  # Training labels\n",
    "\n",
    "X_val = np.empty((0,141,1024))\n",
    "y_val = np.empty((0,))\n",
    "\n",
    "X_test = np.empty((0,141,1024))  # EEG testing data\n",
    "y_test = np.empty((0,))   # Testing labels\n",
    "\n",
    "# Loop through each subject in the training set\n",
    "for subject_file in os.listdir(\"EEGNet/Train\"):\n",
    "    print(f'Now {subject_file} in train')\n",
    "    # Standardize data\n",
    "    epo_std = standardizetr(subject_file)\n",
    "    \n",
    "    # Determine label based on whether the subject is a control or migraineur\n",
    "    if subject_file[0] == 'C':\n",
    "        label = 0  # Control\n",
    "    elif subject_file[0] == 'M':\n",
    "        label = 1  # Migraineur\n",
    "\n",
    "    # Append subject data and labels to the overall training sets\n",
    "    X_train = np.append(X_train, epo_std, axis=0)\n",
    "    y_train = np.append(y_train, [label]*len(epo_std))\n",
    "\n",
    "# Loop through each subject in the training set\n",
    "for subject_file in os.listdir(\"EEGNet/Valid\"):\n",
    "    print(f'Now {subject_file} in train')\n",
    "    # Standardize data\n",
    "    epo_std = standardizeval(subject_file)\n",
    "    \n",
    "    # Determine label based on whether the subject is a control or migraineur\n",
    "    if subject_file[0] == 'C':\n",
    "        label = 0  # Control\n",
    "    elif subject_file[0] == 'M':\n",
    "        label = 1  # Migraineur\n",
    "\n",
    "    # Append subject data and labels to the overall training sets\n",
    "    X_val = np.append(X_val, epo_std, axis=0)\n",
    "    y_val = np.append(y_val, [label]*len(epo_std))\n",
    "\n",
    "# Loop through each subject in the testing set\n",
    "for subject_file in os.listdir(\"EEGNet/Test\"):\n",
    "    print(f'Now {subject_file} in test')\n",
    "    # Standardize data\n",
    "    epo_std = standardizete(subject_file)\n",
    "    \n",
    "    # Determine label based on whether the subject is a control or migraineur\n",
    "    if subject_file[0] == 'C':\n",
    "        label = 0  # Control\n",
    "    else:\n",
    "        label = 1  # Migraineur\n",
    "\n",
    "    # Append subject data and labels to the overall testing sets\n",
    "    X_test = np.append(X_test, epo_std, axis=0)\n",
    "    y_test = np.append(y_test, [label]*len(epo_std))\n",
    "\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_val = y_val.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "np.save('EEGNet/Correct-data/X_train_z.npy', X_train)\n",
    "np.save('EEGNet/Correct-data/X_val_z.npy', X_val)\n",
    "np.save('EEGNet/Correct-data/X_test_z.npy', X_test)\n",
    "np.save('EEGNet/Correct-data/y_train_z.npy', y_train)\n",
    "np.save('EEGNet/Correct-data/y_val_z.npy', y_val)\n",
    "np.save('EEGNet/Correct-data/y_test_z.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = mne.read_epochs('EEGNet/Allfiles/C1-epo.fif')\n",
    "# epoch.pick(['eeg'])\n",
    "\n",
    "# def todataset(data_array, datalabels):\n",
    "#     combined = list(zip(data_array, datalabels))\n",
    "#     np.random.shuffle(combined)\n",
    "#     data_array_shuffled, datalabels_shuffled = zip(*combined)\n",
    "#     del combined\n",
    "#     data_array_shuffled = np.array(data_array_shuffled)\n",
    "#     datalabels_shuffled = np.array(datalabels_shuffled)\n",
    "#     datalabels_shuffled = to_categorical(datalabels_shuffled)\n",
    "#     datalabels_shuffled.astype(np.single)\n",
    "#     print(data_array_shuffled.shape, datalabels_shuffled.shape)\n",
    "#     data_epochs = mne.EpochsArray(data_array_shuffled, info=epoch.info)\n",
    "#     del data_array_shuffled\n",
    "#     datasetxy = create_from_X_y(data_epochs, datalabels, drop_last_window=True, sfreq=512, ch_names=epoch.ch_names)\n",
    "#     return datasetxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_val = np.expand_dims(X_val, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(X_train)\n",
    "X_val = torch.Tensor(X_val)\n",
    "X_test = torch.Tensor(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.Tensor(y_train)\n",
    "y_val = torch.Tensor(y_val)\n",
    "y_test = torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.long()\n",
    "y_val = y_val.long()\n",
    "y_test = y_test.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(X_train, y_train)\n",
    "torch.save(train_data, 'EEGNet/Correct-data/train_data_z.pt')\n",
    "\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "torch.save(val_data, 'EEGNet/Correct-data/val_data_z.pt')\n",
    "\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "torch.save(test_data, 'EEGNet/Correct-data/test_data_z.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = to_categorical(y_train)\n",
    "y_val_cat = to_categorical(y_val)\n",
    "y_test_cat = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = y_train_cat.astype(np.float32)\n",
    "y_val_cat = y_val_cat.astype(np.float32)\n",
    "y_test_cat = y_test_cat.astype(np.float32)\n",
    "\n",
    "y_train_cat = torch.Tensor(y_train_cat)\n",
    "y_val_cat = torch.Tensor(y_val_cat)\n",
    "y_test_cat = torch.Tensor(y_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(X_train, y_train_cat)\n",
    "torch.save(train_data, 'EEGNet/Correct-data/train_data_bce_z.pt')\n",
    "\n",
    "val_data = TensorDataset(X_val, y_val_cat)\n",
    "torch.save(val_data, 'EEGNet/Correct-data/val_data_bce_z.pt')\n",
    "\n",
    "test_data = TensorDataset(X_test, y_test_cat)\n",
    "torch.save(test_data, 'EEGNet/Correct-data/test_data_bce_z.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframes = []\n",
    "test_dataframes = []\n",
    "\n",
    "for file in os.listdir('EEGNet/Trainval'):\n",
    "    shortname = file[:-8] #gives me the exact subject number\n",
    "    raw = mne.read_epochs(f'EEGNet/Trainval/{file}')\n",
    "    raw = raw.pick(None, exclude=['Fp1', 'ECG'])\n",
    "    raw = raw.to_data_frame()\n",
    "    raw = raw.drop(columns=['time', 'condition', 'Status'])\n",
    "    raw = raw.groupby('epoch').mean().reset_index()\n",
    "    raw = raw.drop(columns=['epoch'])\n",
    "    raw = raw * 1e-6\n",
    "    \n",
    "    if shortname[0] == \"M\":\n",
    "        raw['migraine'] = 1\n",
    "    else:\n",
    "        raw['migraine'] = 0\n",
    "\n",
    "    train_dataframes.append(raw)\n",
    "\n",
    "train_df = pd.concat(train_dataframes, ignore_index=True)\n",
    "\n",
    "for file in os.listdir('EEGNet/Test'):\n",
    "    shortname = file[:-8] #gives me the exact subject number\n",
    "    raw = mne.read_epochs(f'EEGNet/Test/{file}')\n",
    "    raw = raw.pick(None, exclude=['Fp1', 'ECG'])\n",
    "    raw = raw.to_data_frame()\n",
    "    raw = raw.drop(columns=['time', 'condition', 'Status'])\n",
    "    raw = raw.groupby('epoch').mean().reset_index()\n",
    "    raw = raw.drop(columns=['epoch'])\n",
    "    raw = raw * 1e-6\n",
    "    \n",
    "    if shortname[0] == \"M\":\n",
    "        raw['migraine'] = 1\n",
    "    else:\n",
    "        raw['migraine'] = 0\n",
    "\n",
    "    test_dataframes.append(raw)\n",
    "\n",
    "test_df = pd.concat(test_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('PreprocessedEpochData/FullDF - Copy.csv', index_col=0)\n",
    "train_subjects = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'M10', 'M11','M12']\n",
    "\n",
    "train_df = df[df['subject'].isin(train_subjects)]\n",
    "test_df = df[~df['subject'].isin(train_subjects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5986159169550173\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67      1584\n",
      "           1       0.58      0.41      0.48      1306\n",
      "\n",
      "    accuracy                           0.60      2890\n",
      "   macro avg       0.59      0.58      0.58      2890\n",
      "weighted avg       0.59      0.60      0.59      2890\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1190  394]\n",
      " [ 766  540]]\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.sample(frac=1)\n",
    "test_df = test_df.sample(frac=1)\n",
    "\n",
    "train_df_x = train_df.drop(columns=['subject','migraine'])\n",
    "train_df_y = train_df['migraine']\n",
    "\n",
    "\n",
    "test_df_x = test_df.drop(columns=['subject','migraine'])\n",
    "test_df_y = test_df['migraine']\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(train_df_x, train_df_y)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = rf_classifier.predict(test_df_x)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = accuracy_score(test_df_y, y_pred)\n",
    "report = classification_report(test_df_y, y_pred)\n",
    "conf_matrix = confusion_matrix(test_df_y, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 69.30%\n",
      "Cross-Validation Scores: [0.69431921 0.70513977 0.68440036 0.69161407 0.68953069]\n",
      "         feature  importance\n",
      "3    delta_AFF5h    0.111994\n",
      "598    gamma_TP7    0.057573\n",
      "568  gamma_AFF1h    0.043851\n",
      "316    alpha_TP7    0.042762\n",
      "627    gamma_Fpz    0.039479\n",
      "567  gamma_AFF5h    0.037492\n",
      "175    theta_TP7    0.036617\n",
      "578  gamma_FFC1h    0.036336\n",
      "586  gamma_FCC5h    0.031211\n",
      "606  gamma_CPP1h    0.030002\n"
     ]
    }
   ],
   "source": [
    "#Perform cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(rf_classifier, train_df_x, train_df_y, cv=cv, scoring='accuracy')\n",
    "\n",
    "print(f'Cross-Validation Accuracy: {np.mean(cv_scores) * 100:.2f}%')\n",
    "print(f'Cross-Validation Scores: {cv_scores}')\n",
    "\n",
    "# Check feature importances\n",
    "importances = rf_classifier.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'feature': train_df_x.columns, 'importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "print(feature_importance_df.head(10))  # Print top 10 features\n",
    "\n",
    "# Plot feature importances\n",
    "feature_importance_df.set_index('feature').head(10).plot(kind='bar')\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()\n",
    "\n",
    "# Visual inspection of top features\n",
    "top_features = feature_importance_df['feature'].head(3)\n",
    "for feature in top_features:\n",
    "    sns.scatterplot(data=df, x=feature, y='migraine')\n",
    "    plt.title(f'{feature} vs Migraine')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7063492063492064\n",
      "Test accuracy: 0.5986159169550173\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy:\", rf_classifier.score(train_df_x, train_df_y))\n",
    "print(\"Test accuracy:\", rf_classifier.score(test_df_x, test_df_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         feature  importance_mean  importance_std\n",
      "634  gamma_AFF6h         0.070542        0.007563\n",
      "598    gamma_TP7         0.013057        0.003568\n",
      "627    gamma_Fpz         0.008512        0.002992\n",
      "694    gamma_LO2         0.004971        0.001871\n",
      "625     gamma_Oz         0.004048        0.002475\n",
      "..           ...              ...             ...\n",
      "584  gamma_FTT9h        -0.001027        0.000209\n",
      "693    gamma_LO1        -0.001407        0.000218\n",
      "697    gamma_IO2        -0.002215        0.000510\n",
      "695    gamma_IO1        -0.002249        0.000343\n",
      "620  gamma_POO9h        -0.009942        0.003219\n",
      "\n",
      "[705 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "# Compute permutation importance\n",
    "perm_importance = permutation_importance(rf_classifier, test_df_x, test_df_y, n_repeats=30, random_state=42)\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'feature': test_df_x.columns,\n",
    "    'importance_mean': perm_importance.importances_mean,\n",
    "    'importance_std': perm_importance.importances_std\n",
    "}).sort_values(by='importance_mean', ascending=False)\n",
    "\n",
    "print(perm_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M2       0.010150\n",
      "FFT8h    0.006757\n",
      "F2       0.004213\n",
      "LO2      0.003175\n",
      "C2       0.002756\n",
      "           ...   \n",
      "AFz     -0.004517\n",
      "FCC5h   -0.004982\n",
      "F6      -0.005326\n",
      "M1      -0.007764\n",
      "C1      -0.008112\n",
      "Name: migraine, Length: 141, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate correlation with the target\n",
    "correlation_with_target = train_df.corr()['migraine'].drop('migraine')\n",
    "\n",
    "print(correlation_with_target.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visual inspection of top features\n",
    "top_features = feature_importance_df['feature'].head(3)  # Adjust the number of features as needed\n",
    "\n",
    "for feature in top_features:\n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=df, x=feature, y='migraine', hue='migraine', alpha=0.6)\n",
    "    plt.title(f'Scatter plot of {feature} vs Migraine')\n",
    "    plt.show()\n",
    "    \n",
    "    # Histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=df, x=feature, hue='migraine', multiple='stack', kde=True)\n",
    "    plt.title(f'Histogram of {feature} for Migraine and Non-Migraine')\n",
    "    plt.show()\n",
    "\n",
    "# Pairplot for top features (optional for inspecting interactions)\n",
    "sns.pairplot(df, vars=top_features, hue='migraine', plot_kws={'alpha': 0.6})\n",
    "plt.suptitle('Pairplot of Top Features', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('GroupedDF.csv', index_col='index')\n",
    "# sample_df = df.sample(frac=0.2, random_state=42)\n",
    "# X = sample_df.drop(columns=['subject', 'migraine'])\n",
    "# y = sample_df['migraine']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))),\n",
    "  ('classification', RandomForestClassifier())\n",
    "])\n",
    "clf.fit(train_df_x, train_df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(test_df_x)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(test_df_y, y_pred)\n",
    "report = classification_report(test_df_y, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('PreprocessedEpochData/X_train.npy')\n",
    "X_val = np.load('PreprocessedEpochData/X_val.npy')\n",
    "X_test = np.load('PreprocessedEpochData/X_test.npy')\n",
    "\n",
    "y_train = np.load('PreprocessedEpochData/y_train.npy')\n",
    "y_val = np.load('PreprocessedEpochData/y_val.npy')\n",
    "y_test = np.load('PreprocessedEpochData/y_test.npy')\n",
    "device= 'cuda'\n",
    "\n",
    "# Set n_channels to 143 and n_times to 512 for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('EEGNet/Correct-data/X_train.npy')\n",
    "X_val = np.load('EEGNet/Correct-data/X_val.npy')\n",
    "X_test = np.load('EEGNet/Correct-data/X_test.npy')\n",
    "\n",
    "y_train = np.load('EEGNet/Correct-data/y_train.npy')\n",
    "y_val = np.load('EEGNet/Correct-data/y_val.npy')\n",
    "y_test = np.load('EEGNet/Correct-data/y_test.npy')\n",
    "device= 'cuda'\n",
    "\n",
    "# Set n_channels to 141 and n_times to 1024 for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Data in Formate (EEG-Epochs,Channels,Timepoints) and label file with 1/0\n",
      "Train: (5544, 141, 1024) (5544,)\n",
      "Valid: (1375, 141, 1024) (1375,)\n",
      "Test: (1515, 141, 1024) (1515,) \n",
      "\n",
      "Ratio of labels 1 and 0:\n",
      "Train:[1.02666667 0.97468354]\n",
      "Valid: [0.87915601 1.15935919]\n",
      "Test: [0.94451372 1.06241234] \n",
      "\n",
      "Label 1 in Training data: 2844\n",
      "Label 0 in Training data: 2700\n",
      "Label 1 in Validation data: 593\n",
      "Label 0 in Validation data: 782\n",
      "Label 1 in Test data: 713\n",
      "Label 0 in Test data: 802\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "print('Shape of Data in Formate (EEG-Epochs,Channels,Timepoints) and label file with 1/0')\n",
    "print(f'Train: {X_train.shape} {y_train.shape}')\n",
    "print(f'Valid: {X_val.shape} {y_val.shape}')\n",
    "print(f'Test: {X_test.shape} {y_test.shape} \\n')\n",
    "\n",
    "print('Ratio of labels 1 and 0:')\n",
    "print(f'Train:{compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)}')\n",
    "print(f'Valid: {compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_val), y=y_val)}')\n",
    "print(f'Test: {compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_test), y=y_test)} \\n')\n",
    "\n",
    "print(f'Label 1 in Training data: {np.count_nonzero(y_train)}')\n",
    "print(f'Label 0 in Training data: {len(y_train) - np.count_nonzero(y_train)}')\n",
    "print(f'Label 1 in Validation data: {np.count_nonzero(y_val)}')\n",
    "print(f'Label 0 in Validation data: {len(y_val) - np.count_nonzero(y_val)}')\n",
    "print(f'Label 1 in Test data: {np.count_nonzero(y_test)}')\n",
    "print(f'Label 0 in Test data: {len(y_test) - np.count_nonzero(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Shuffler\n",
    "n_samples = len(X_train)\n",
    "index = np.arange(n_samples)\n",
    "np.random.shuffle(index)\n",
    "\n",
    "# Shuffle data based on shuffled indices\n",
    "X_train = X_train[index]\n",
    "y_train = y_train[index]\n",
    "\n",
    "# n_samples = len(X_val)\n",
    "# index = np.arange(n_samples)\n",
    "# np.random.shuffle(index)\n",
    "\n",
    "# X_val = X_val[index]\n",
    "# y_val = y_val[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# Assuming y_train is a numpy array\n",
    "X_train_tensor = torch.tensor(X_train).float()\n",
    "X_val_tensor = torch.tensor(X_val).float()\n",
    "y_train_tensor = torch.tensor(y_train).float()\n",
    "y_val_tensor = torch.tensor(y_val).float()\n",
    "X_test_tensor = torch.tensor(X_test).float()\n",
    "y_test_tensor = torch.tensor(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "train_weight = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "train_weight = torch.tensor([train_weight[0] , train_weight[1]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train f1-score    train_acc    train_loss    valid f1-score    valid_acc    valid_loss      dur\n",
      "-------  ----------------  -----------  ------------  ----------------  -----------  ------------  -------\n",
      "      1            \u001b[36m0.4811\u001b[0m       \u001b[32m0.4926\u001b[0m        \u001b[35m0.7795\u001b[0m            \u001b[31m0.4282\u001b[0m       \u001b[94m0.5505\u001b[0m        \u001b[36m0.7153\u001b[0m  22.3643\n",
      "      2            \u001b[36m0.4970\u001b[0m       \u001b[32m0.5041\u001b[0m        \u001b[35m0.7413\u001b[0m            0.4102       \u001b[94m0.5729\u001b[0m        \u001b[36m0.7087\u001b[0m  19.7215\n",
      "      3            \u001b[36m0.5029\u001b[0m       \u001b[32m0.5079\u001b[0m        \u001b[35m0.7375\u001b[0m            0.4015       \u001b[94m0.5828\u001b[0m        \u001b[36m0.7002\u001b[0m  20.0960\n",
      "      4            \u001b[36m0.5173\u001b[0m       \u001b[32m0.5256\u001b[0m        \u001b[35m0.7103\u001b[0m            0.3831       \u001b[94m0.5960\u001b[0m        \u001b[36m0.6963\u001b[0m  19.9307\n",
      "      5            \u001b[36m0.5240\u001b[0m       0.5245        \u001b[35m0.7051\u001b[0m            0.3753       \u001b[94m0.6000\u001b[0m        \u001b[36m0.6925\u001b[0m  20.8454\n",
      "      6            \u001b[36m0.5381\u001b[0m       \u001b[32m0.5494\u001b[0m        \u001b[35m0.6903\u001b[0m            0.3541       0.5954        0.6938  21.5882\n",
      "      7            0.5328       0.5426        \u001b[35m0.6875\u001b[0m            0.3634       0.6000        0.6969  19.4672\n",
      "      8            \u001b[36m0.5388\u001b[0m       \u001b[32m0.5503\u001b[0m        \u001b[35m0.6846\u001b[0m            0.3600       0.5941        0.7022  18.9470\n",
      "      9            \u001b[36m0.5391\u001b[0m       \u001b[32m0.5565\u001b[0m        \u001b[35m0.6745\u001b[0m            0.3564       0.5947        0.7053  18.9196\n",
      "Restoring best model from epoch 5.\n"
     ]
    }
   ],
   "source": [
    "from skorch.dataset import Dataset as skorchset\n",
    "from skorch.classifier import NeuralNetBinaryClassifier\n",
    "from skorch.callbacks import EpochScoring\n",
    "eeg_model = EEGNetv4(n_chans=141, n_outputs=1, n_times=1024, kernel_length=256, drop_prob=0.4, F1=8, D=4, F2=32)\n",
    "opt = optim.Adam(eeg_model.parameters(), lr=0.00005, weight_decay=0.0001, eps=1e-8, betas=(0.9, 0.999))\n",
    "eeg_model.cuda()\n",
    "n_epochs = 300\n",
    "\n",
    "# class WeightedBCELoss(nn.BCEWithLogitsLoss):\n",
    "#     def __init__(self, weight):\n",
    "#         super(WeightedBCELoss, self).__init__(pos_weight=weight[1]/weight[0])\n",
    "\n",
    "eeg_net = NeuralNetBinaryClassifier(eeg_model,\n",
    "                        criterion=nn.BCEWithLogitsLoss,\n",
    "                        #criterion__weight = train_weight,\n",
    "                        optimizer=optim.NAdam,\n",
    "                        #threshold=0.465,\n",
    "                        optimizer__lr=0.00001,\n",
    "                        optimizer__weight_decay = 0.1,\n",
    "                        optimizer__eps = 1e-8,\n",
    "                        optimizer__betas = (0.9, 0.999),\n",
    "                        max_epochs=n_epochs,\n",
    "                        train_split = None,\n",
    "                        iterator_train__shuffle=True,\n",
    "                        device='cuda', \n",
    "                        batch_size=32,\n",
    "                        callbacks=[\n",
    "                            (EpochScoring(scoring='accuracy', name='train_acc', on_train=True, lower_is_better=False)),\n",
    "                            (EpochScoring(scoring='f1', name='train f1-score', on_train=True, lower_is_better=False)),\n",
    "                            (EpochScoring(scoring='f1', name='valid f1-score', on_train=False, lower_is_better=False)),\n",
    "                            (\"lr_schedulerPlat\", LRScheduler('ReduceLROnPlateau', patience=5, factor=0.1)),\n",
    "                            #(\"lr_schedulerStep\", LRScheduler('StepLR', step_size=5, gamma=0.8)),\n",
    "                            #(\"lr_schedulerExp\", LRScheduler('ExponentialLR', gamma=0.9)),\n",
    "                            ('EarlyStopping', EarlyStopping(patience=100, load_best=True, lower_is_better=True))\n",
    "                        ]\n",
    "                        )\n",
    "\n",
    "val_data = skorchset(X_test_tensor, y_test_tensor)\n",
    "eeg_net.set_params(train_split = predefined_split(val_data))\n",
    "eeg_fit = eeg_net.fit(X_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.58      0.90      0.71       782\n",
      "         1.0       0.54      0.15      0.23       593\n",
      "\n",
      "    accuracy                           0.58      1375\n",
      "   macro avg       0.56      0.53      0.47      1375\n",
      "weighted avg       0.56      0.58      0.50      1375\n",
      "\n",
      "Test Accuracy: 0.5774545454545454\n",
      "Test F1-Score: 0.23249669749009247\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "y_pred = eeg_net.predict(X_val_tensor)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "print(classification_report(y_val_tensor, y_pred_classes))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_val_tensor, y_pred_classes))\n",
    "print('Test F1-Score:', f1_score(y_pred=y_pred_classes, y_true=y_val_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    f1-score    train_acc    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ----------  -----------  ------------  -----------  ------------  ------\n",
      "      1      \u001b[36m0.6744\u001b[0m       \u001b[32m0.6124\u001b[0m        \u001b[35m0.6659\u001b[0m       \u001b[31m0.5822\u001b[0m        \u001b[94m1.3160\u001b[0m  2.3150\n",
      "      2      0.6686       \u001b[32m0.6733\u001b[0m        \u001b[35m0.5895\u001b[0m       0.5624        1.3218  1.7782\n",
      "      3      \u001b[36m0.6820\u001b[0m       \u001b[32m0.7008\u001b[0m        \u001b[35m0.5559\u001b[0m       \u001b[31m0.5894\u001b[0m        1.5361  1.6684\n",
      "      4      \u001b[36m0.7017\u001b[0m       \u001b[32m0.7249\u001b[0m        \u001b[35m0.5314\u001b[0m       0.5894        1.9916  1.5329\n",
      "      5      \u001b[36m0.7214\u001b[0m       \u001b[32m0.7381\u001b[0m        \u001b[35m0.5218\u001b[0m       \u001b[31m0.5914\u001b[0m        1.6260  1.5224\n",
      "      6      \u001b[36m0.7345\u001b[0m       \u001b[32m0.7527\u001b[0m        \u001b[35m0.4970\u001b[0m       0.5769        1.4716  1.5758\n",
      "      7      \u001b[36m0.7422\u001b[0m       \u001b[32m0.7594\u001b[0m        \u001b[35m0.4880\u001b[0m       0.5881        2.6652  1.5984\n",
      "      8      \u001b[36m0.7567\u001b[0m       \u001b[32m0.7695\u001b[0m        \u001b[35m0.4731\u001b[0m       \u001b[31m0.5921\u001b[0m        1.9445  1.4544\n",
      "      9      \u001b[36m0.7721\u001b[0m       \u001b[32m0.7832\u001b[0m        \u001b[35m0.4598\u001b[0m       0.5875        2.7387  1.4725\n",
      "     10      \u001b[36m0.7875\u001b[0m       \u001b[32m0.7983\u001b[0m        \u001b[35m0.4469\u001b[0m       0.5802        1.5091  1.4807\n",
      "     11      \u001b[36m0.7945\u001b[0m       \u001b[32m0.8047\u001b[0m        \u001b[35m0.4384\u001b[0m       0.5868        2.6796  1.4658\n",
      "     12      \u001b[36m0.8048\u001b[0m       \u001b[32m0.8140\u001b[0m        \u001b[35m0.4247\u001b[0m       0.5875        3.2144  1.4964\n",
      "     13      \u001b[36m0.8101\u001b[0m       \u001b[32m0.8162\u001b[0m        \u001b[35m0.4207\u001b[0m       \u001b[31m0.5927\u001b[0m        1.9769  1.4449\n",
      "Restoring best model from epoch 1.\n"
     ]
    }
   ],
   "source": [
    "deep_model = Deep4Net(n_chans=143, n_outputs=1, n_times=512, drop_prob=0.25, add_log_softmax=False)\n",
    "deep_model.cuda()\n",
    "n_epochs = 300\n",
    "deep_net = NeuralNetBinaryClassifier(deep_model,\n",
    "                        optimizer=optim.Adamax,\n",
    "                        threshold=0.465,\n",
    "                        optimizer__lr=0.00002,\n",
    "                        #optimizer__weight_decay = 1e-6,\n",
    "                        max_epochs=n_epochs,\n",
    "                        train_split = None,\n",
    "                        iterator_train__shuffle=True,\n",
    "                        device='cuda', \n",
    "                        batch_size=32,\n",
    "                        callbacks=[\n",
    "                            (EpochScoring(scoring='accuracy', name='train_acc', on_train=True, lower_is_better=False)),\n",
    "                            (EpochScoring(scoring='f1', name='f1-score', on_train=True, lower_is_better=False)),\n",
    "                            #(\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "                            ('EarlyStopping', EarlyStopping(patience=50, load_best=True))\n",
    "                        ]\n",
    "                        )\n",
    "\n",
    "val_data = skorchset(X_val_tensor, y_val_tensor)\n",
    "deep_net.set_params(train_split = predefined_split(val_data))\n",
    "deep_fit = deep_net.fit(X_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4930909090909091\n",
      "Test F1-Score: 0.4967509025270758\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "X_test_tensor = torch.tensor(X_test).float()\n",
    "y_test_tensor = torch.tensor(y_test).float()\n",
    "y_pred = deep_net.predict(X_test_tensor)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test_tensor, y_pred))\n",
    "print('Test F1-Score:', f1_score(y_pred=y_pred, y_true=y_test))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model = Deep4Net(n_chans=144, n_classes=2, n_times=1024, drop_prob=0.25)\n",
    "\n",
    "deep_net = EEGClassifier(deep_model,\n",
    "                        optimizer = optim.Adam,\n",
    "                        optimizer__lr = 0.01,\n",
    "                        optimizer__weight_decay = 0.0005,\n",
    "                        train_split=ValidSplit(0.2), \n",
    "                        max_epochs=n_epochs, \n",
    "                        device='cuda', \n",
    "                        batch_size=32,\n",
    "                        callbacks=[\n",
    "                            \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "                        ],\n",
    "                        classes=[0,1]\n",
    "                        )\n",
    "deep_net.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_model = partial(EEGNetv4, n_chans=141, n_outputs=2, n_times=1024, kernel_length=256, drop_prob=0.25)\n",
    "\n",
    "grid_net = EEGClassifier(grid_model,\n",
    "                    optimizer = optim.Adam,\n",
    "                    optimizer__lr = [],\n",
    "                    train_split= ValidSplit(0.2),\n",
    "                    device='cuda', \n",
    "                    batch_size=64,\n",
    "                    callbacks=[\n",
    "                        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=30 - 1)),\n",
    "                    ],\n",
    "                    classes=[0,1]\n",
    "                    )\n",
    "\n",
    "# train_X = SliceDataset(train_dataset, idx=0)\n",
    "# train_y = array([y for y in SliceDataset(train_dataset, idx=1)])\n",
    "cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "learning_rates = [0.001, 0.0001, 0.00005]\n",
    "drop_probs = [0.3, 0.5]\n",
    "\n",
    "fit_params = {'epochs': 30}\n",
    "param_grid = {\n",
    "    'optimizer__lr': learning_rates,\n",
    "    'module__drop_prob': drop_probs\n",
    "}\n",
    "\n",
    "# By setting n_jobs=-1, grid search is performed\n",
    "# with all the processors, in this case the output of the training\n",
    "# process is not printed sequentially\n",
    "search = GridSearchCV(\n",
    "    estimator=grid_net,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    return_train_score=True,\n",
    "    scoring='accuracy',\n",
    "    refit=True,\n",
    "    verbose=3,\n",
    "    error_score='raise',\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "# Extract the results into a DataFrame\n",
    "search_results = pd.DataFrame(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table for the heatmap\n",
    "pivot_table = search_results.pivot(index='param_optimizer__lr',\n",
    "                                   columns='param_module__drop_prob',\n",
    "                                   values='mean_test_score')\n",
    "# Create the heatmap\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(pivot_table, annot=True, fmt=\".3f\",\n",
    "            cmap=\"YlGnBu\", cbar=True)\n",
    "plt.title('Grid Search Mean Test Scores')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.xlabel('Dropout Probability')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = search_results[search_results['rank_test_score'] == 1].squeeze()\n",
    "print(\n",
    "    f\"Best hyperparameters were {best_run['params']} which gave a validation \"\n",
    "    f\"accuracy of {best_run['mean_test_score'] * 100:.2f}% (training \"\n",
    "    f\"accuracy of {best_run['mean_train_score'] * 100:.2f}%).\")\n",
    "\n",
    "eval_X = SliceDataset(test_dataset, idx=0)\n",
    "eval_y = SliceDataset(test_dataset, idx=1)\n",
    "score = search.score(eval_X, eval_y)\n",
    "print(f\"Test accuracy is {score * 100:.2f}%.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
